##############################################################################
#
#   MRC FGU CGAT
#
#   $Id$
#
#   Copyright (C) 2009 Andreas Heger
#
#   This program is free software; you can redistribute it and/or
#   modify it under the terms of the GNU General Public License
#   as published by the Free Software Foundation; either version 2
#   of the License, or (at your option) any later version.
#
#   This program is distributed in the hope that it will be useful,
#   but WITHOUT ANY WARRANTY; without even the implied warranty of
#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#   GNU General Public License for more details.
#
#   You should have received a copy of the GNU General Public License
#   along with this program; if not, write to the Free Software
#   Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
###############################################################################
"""===========================
Pipeline template
===========================

:Author: Andreas Heger
:Release: $Id$
:Date: |today|
:Tags: Python

.. Replace the documentation below with your own description of the
   pipeline's purpose

Overview
========

This pipeline computes the word frequencies in the configuration
files :file:``pipeline.ini` and :file:`conf.py`.

Usage
=====

See :ref:`PipelineSettingUp` and :ref:`PipelineRunning` on general
information how to use CGAT pipelines.

Configuration
-------------

The pipeline requires a configured :file:`pipeline.ini` file.
CGATReport report requires a :file:`conf.py` and optionally a
:file:`cgatreport.ini` file (see :ref:`PipelineReporting`).

Default configuration files can be generated by executing:

   python <srcdir>/pipeline_macs2.py config

Input files
-----------
- Bam files & bai indexes

- sample files should be of format: <exp>-<cond>-<treatment>-<sample>.genome.bam
- with matching file for input:     <exp>-<cond>-<treatment>-<WCE>.genome.bam
- if only 1 input is needed for all samples it can be specified in pipeline.ini

Requirements
------------

The pipeline requires the results from
:doc:`pipeline_annotations`. Set the configuration variable
:py:data:`annotations_database` and :py:data:`annotations_dir`.

On top of the default CGAT setup, the pipeline requires the following
software to be in the path:

.. Add any additional external requirements such as 3rd party software
   or R modules below:

Requirements:

* samtools >= 1.1

Pipeline output
===============

.. Describe output files of the pipeline here

Glossary
========

.. glossary::


Code
====

"""
from ruffus import *

import sys
import os
import sqlite3
import CGAT.Experiment as E
import CGATPipelines.Pipeline as P
import CGAT.BamTools as BamTools
import CGAT.Database as DB
import re
import glob
import gzip
import pandas as pd
import numpy as np
from pybedtools import BedTool
import seaborn as sns
from matplotlib import pyplot as plt

# load options from the config file
PARAMS = P.getParameters(
    ["%s/pipeline.ini" % os.path.splitext(__file__)[0],
     "../pipeline.ini",
     "pipeline.ini"])

# add configuration values from associated pipelines
#
# 1. pipeline_annotations: any parameters will be added with the
#    prefix "annotations_". The interface will be updated with
#    "annotations_dir" to point to the absolute path names.
PARAMS.update(P.peekParameters(
    PARAMS["annotations_dir"],
    "pipeline_annotations.py",
    on_error_raise=__name__ == "__main__",
    prefix="annotations_",
    update_interface=True))


# if necessary, update the PARAMS dictionary in any modules file.
# e.g.:
#
# import CGATPipelines.PipelineGeneset as PipelineGeneset
# PipelineGeneset.PARAMS = PARAMS
#
# Note that this is a hack and deprecated, better pass all
# parameters that are needed by a function explicitely.

# -----------------------------------------------
# Utility functions
def connect():
    '''utility function to connect to database.

    Use this method to connect to the pipeline database.
    Additional databases can be attached here as well.

    Returns an sqlite3 database handle.
    '''

    dbh = sqlite3.connect(PARAMS["database"])
    statement = '''ATTACH DATABASE '%s' as annotations''' % (
        PARAMS["annotations_database"])
    cc = dbh.cursor()
    cc.execute(statement)
    cc.close()

    return dbh


def isPaired(files):
    '''Check whether input files are single or paired end
       Note: this is dependent on files having correct suffix'''
    
    paired = []

    for fastq in files:
        Fpair = re.findall(".*.fastq.1.gz", fastq)
        paired = paired + Fpair

    if len(paired)==0:
        unpaired = True

    else:
        unpaired = False
    
    return unpaired


def writeGreat(locations,basalup,basaldown,maxext,outfile,half=False):
    ''' write out a bed file of great promoters from input gene locations
         locations is [contig,gstart,gend,strand,gene_id] '''

    # Gene regulatory domain definition: 
    # Each gene is assigned a basal regulatory domain of a 
    # minimum distance upstream and downstream of the TSS 
    # (regardless of other nearby genes). 
    # The gene regulatory domain is extended in both directions 
    # to the nearest gene's basal domain but no more than the 
    # maximum extension in one direction

    genome = {}
    for location in locations:
        chrom, gstart, gend, strand_int, gid = location
        if strand_int == -1: 
            strand = "minus" 
            tss = gend
        else: 
            strand = "plus"
            tss = gstart
        record = [tss,strand,gid]
        if chrom[3:5]=="NT" or chrom[0:2]=="MT": continue
        if chrom not in genome: 
            genome[chrom] = [ record ]
        else: genome[chrom].append(record)

    #add the ends of the chromosomes
    contigs = gzip.open(PARAMS["annotations_dir"]+"/assembly.dir/contigs.bed.gz","r")

    
    nmatched = 0
    for contig_entry in contigs:
        contig, start, end = contig_entry.strip().split("\t")
        
        if contig in genome.keys():
            genome[contig].append([int(end),"end","end"])
            nmatched+=1
    if nmatched < 21:
        raise ValueError("writeGreat: not enough chromosome ends registered")

    #sort the arrays
    for key in genome.keys():
        genome[key].sort( key = lambda entry: entry[0] )
        
    #now we can walk over the regions and make the regulatory regions.

    greatBed = []
   
    for contig in genome.keys():

        locs = genome[contig]
        contig_end = locs[-1][0]
        for i in range(0,len(locs)):

            l,strand,gid = locs[i]

            if strand == "end": continue

            #get the positions of the neighbouring basal domains.

            # - upstream
            if i == 0: frontstop = 0
            else:
                pl, pstrand, pgid = locs[i-1]
                if pstrand == "plus": frontstop = pl + basaldown
                else: frontstop = pl + basalup
            # - downstream
            nl, nstrand, ngid = locs[i+1]
            if nstrand == "plus": backstop = nl - basalup
            else: backstop = nl - basaldown

            # define basal domain
            if strand=="plus":
                basalstart = l - basalup
                basalend = min( l + basaldown, contig_end )
            else:
                basalstart = l - basaldown
                basalend = min( l + basalup, contig_end )

            # upstream extension
            if frontstop > basalstart:
                regstart = basalstart
            else:
                if half == True:
                    frontext = min( maxext, (l - frontstop) / 2 )
                else:
                    frontext = min( maxext, l - frontstop )
                regstart = l - frontext

            # downstream extension
            if backstop < basalend:
                regend = basalend
            else:
                if half == True:
                    backext = min( maxext, ( backstop - l ) / 2 )
                else:
                    backext = min( maxext, backstop - l )
                regend = l + backext

            greatBed.append(["chr"+contig,str(regstart),str(regend),gid])
        
    outfh = open(outfile,"w")
    outfh.write("\n".join(["\t".join(x) for x in greatBed])+"\n")
    outfh.close()



def getTSS(start,end,strand):
    if strand == 1 or strand == "+": tss = start
    elif strand == -1 or strand == "-": tss = end
    else: raise ValueError("getTSS: stand specification not understood")
    return tss

# ---------------------------------------------------
# Specific pipeline tasks

# Configure pipeline for paired or single end data
Unpaired = isPaired(glob.glob("data.dir/*fastq*gz"))


#####################################################
####                Mapping                      ####
#####################################################
@follows(connect, mkdir("bowtie2.dir"))
@transform("data.dir/*.fastq.1.gz",
           regex(r"data.dir/(.*).fastq.1.gz"),
           r"bowtie2.dir/\1.genome.bam")
def mapBowtie2_PE(infile, outfile):
    '''Map reads with Bowtie2'''

    if len(infile) == 0:
        pass

    read1 = infile
    read2 = infile.replace(".1.gz", ".2.gz")
    
    job_memory = "2G"
    job_threads = "12"
    
    log = outfile + "_bowtie2.log"
    tmp_dir = "$SCRATCH_DIR"

    options = PARAMS["bowtie2_options"]
    genome = os.path.join(PARAMS["bowtie2_genomedir"], PARAMS["bowtie2_genome"])

    statement = '''tmp=`mktemp -p %(tmp_dir)s`; checkpoint;
                   bowtie2 
                     --quiet 
                     --threads 12 
                     -x %(genome)s
                     -1 %(read1)s 
                     -2 %(read2)s
                     %(options)s
                     1> $tmp 
                     2> %(log)s; checkpoint;
                   samtools sort -O BAM -o %(outfile)s $tmp; checkpoint;
                   samtools index %(outfile)s; checkpoint;
                   rm $tmp''' % locals()

    print statement

    P.run()


@active_if(Unpaired)
@transform("data.dir/*.fastq.gz",
           regex(r"data.dir/(.*).fastq.gz"),
           r"bowtie2.dir/\1.genome.bam")
def mapBowtie2_SE(infile, outfile):
    '''Map reads with Bowtie2'''

    if len(infile) == 0:
        pass

    job_memory = "2G"
    job_threads = "12"
    
    log = outfile + "_bowtie2.log"
    tmp_dir = "$SCRATCH_DIR"

    options = PARAMS["bowtie2_options"]
    genome = os.path.join(PARAMS["bowtie2_genomedir"], PARAMS["bowtie2_genome"])

    statement = '''tmp=`mktemp -p %(tmp_dir)s`; checkpoint;
                   bowtie2 
                     --quiet 
                     --threads 12 
                     -x %(genome)s
                     -U %(infile)s 
                     %(options)s
                     1> $tmp 
                     2> %(log)s; checkpoint;
                   samtools sort -O BAM -o %(outfile)s $tmp; checkpoint;
                   samtools index %(outfile)s; checkpoint;
                   rm $tmp''' % locals()

    print statement

    P.run()

@follows(mapBowtie2_PE, mapBowtie2_SE)
@transform("bowtie2.dir/*.genome.bam", suffix(r".genome.bam"), r".filt.bam")
def filterBam(infile, outfile):
    '''filter bams on MAPQ >10, & remove reads mapping to chrM before peakcalling'''

    job_memory = "10G"
    job_threads = "2"

    local_tmpdir = "$SCRATCH_DIR"
        
    statement = '''tmp=`mktemp -p %(local_tmpdir)s`; checkpoint ;
                   head=`mktemp -p %(local_tmpdir)s`; checkpoint ;
                   samtools view -h %(infile)s | grep "^@" - > $head ; checkpoint ; 
                   samtools view -q10 %(infile)s | 
                     grep -v "chrM" - | 
                     cat $head - |
                     samtools view -h -o $tmp - ; checkpoint ;
                   samtools sort -O BAM -o %(outfile)s $tmp ; checkpoint ;
                   samtools index %(outfile)s; checkpoint;
                   rm $tmp $head''' % locals()

    print statement

    P.run()
    

@transform(filterBam,
           regex(r"(.*).filt.bam"),
           r"\1.prep.bam")
def removeDuplicates(infile, outfile):
    '''PicardTools remove duplicates'''

    job_memory = "5G"
    job_threads = "2"
    
    metrics_file = outfile + ".picardmetrics"
    log = outfile + ".picardlog"
    tmp_dir = "$SCRATCH_DIR"

    statement = '''tmp=`mktemp -p %(tmp_dir)s`; checkpoint ; 
                   MarkDuplicates 
                     INPUT=%(infile)s 
                     ASSUME_SORTED=true 
                     REMOVE_DUPLICATES=true 
                     QUIET=true 
                     OUTPUT=$tmp 
                     METRICS_FILE=%(metrics_file)s 
                     VALIDATION_STRINGENCY=SILENT
                     2> %(log)s ; checkpoint ;
                   mv $tmp %(outfile)s; checkpoint ; 
                   samtools index %(outfile)s'''

    print statement

    P.run()

    
@active_if(Unpaired == False)
@transform(removeDuplicates,
           suffix(r".prep.bam"),
           r".size_filt.prep.bam")
def size_filterBam(infile, outfile):
    '''filter bams on insert size (max size specified in ini)'''

    job_memory = "10G"
    job_threads = "2"

    local_tmpdir = "$SCRATCH_DIR"

    insert_size_filter_F = PARAMS["bowtie2_insert_size"]
    insert_size_filter_R = "-" + str(insert_size_filter_F) # reverse reads have "-" prefix for TLEN

    statement = '''tmp=`mktemp -p %(local_tmpdir)s`; checkpoint ;
                   head=`mktemp -p %(local_tmpdir)s`; checkpoint ;
                   samtools view -h %(infile)s | grep "^@" - > $head ; checkpoint ; 
                   samtools view %(infile)s | 
                     awk 'BEGIN {OFS="\\t"} {if ($9 ~ /^-/ && $9 > %(insert_size_filter_R)s) print $0;
                       else if ($9 ~ /^[0-9]/ && $9 < %(insert_size_filter_F)s) print $0}' - |     
                     cat $head - |
                     samtools view -h -o $tmp - ; checkpoint ;
                   samtools sort -O BAM -o %(outfile)s $tmp ; checkpoint ;
                   samtools index %(outfile)s; checkpoint;
                   rm $tmp $head''' % locals()


    print statement

    P.run()

    
@follows(size_filterBam)
@transform("bowtie2.dir/*.bam", suffix(r".bam"), r".bam.bai")
def indexBam(infile, outfile):
    '''index bams, if index failed to be generated'''

    statement = '''samtools index -b %(infile)s > %(outfile)s'''

    P.run()

    
####################################################
#####               Mapping QC                 #####
####################################################
@follows(indexBam)
@transform("bowtie2.dir/*.genome.bam",
           regex(r"(.*).genome.bam"),
           r"\1.contigs.counts")
def contigReadCounts(infile, outfile):
    '''count reads mapped to each contig'''

    tmp_dir = "$SCRATCH_DIR"
    name = os.path.basename(infile).rstrip(".bam")
    
    statement =  '''tmp=`mktemp -p %(tmp_dir)s`; checkpoint ;
                    samtools idxstats %(infile)s > $tmp; checkpoint;
                    awk 'BEGIN {OFS="\\t"} {print $0,"%(name)s"}' $tmp > %(outfile)s; checkpoint;
                    rm $tmp'''

    print statement

    P.run()


@follows(contigReadCounts)
@merge("bowtie2.dir/*.contigs.counts", "allContig.counts")
def mergeContigCounts(infiles, outfile):

    infiles = ' '.join(infiles)
    
    statement = '''cat %(infiles)s > %(outfile)s'''

    P.run()

@transform(mergeContigCounts, suffix(r".counts"), r".load")
def loadmergeContigCounts(infile, outfile):
    P.load(infile, outfile, options='-H "contig,length,mapped_reads,unmapped_reads,sample_id" ')

    
@follows(loadmergeContigCounts)
@transform("bowtie2.dir/*.bam", suffix(r".bam"), r".flagstats.txt")
def flagstatBam(infile, outfile):
    '''get samtools flagstats for bams'''

    statement = '''samtools flagstat %(infile)s > %(outfile)s'''

    print statement
    
    P.run()


@merge(flagstatBam, "bowtie2.dir/flagstats.load")
def loadflagstatBam(infiles, outfile):
    '''Summarize & load samtools flagstats'''
    
    n = 0

    for infile in infiles:

        n = n + 1
        
        QC_passed = []
        QC_failed = []
        cat = ['total', 'secondary', 'supplementary', 'duplicates', 'mapped', 'paired_in_sequencing', 'read1', 'read2',
                'properly_paired', 'itself_and_mate_mapped', 'singletons', 'mate_mapped_2_diff_chr', 'mate_mapped_2_diff_chr_and_MAPQ_5+']

        name = '_'.join(os.path.basename(infile).split(".")[0:2])

        with open(infile, "r") as o:
            for line in o:
                QC_passed.append(line.split(" ")[0])
                QC_failed.append(line.split(" ")[2])

        QCpass = dict(zip(cat, QC_passed))
        QCfail = dict(zip(cat, QC_failed))

        pass_df = pd.DataFrame.from_dict(QCpass, orient="index").transpose()
        pass_df["QC_status"] = "pass"

        fail_df = pd.DataFrame.from_dict(QCfail, orient="index").transpose()
        fail_df["QC_status"] = "fail"

        if n == 1:
            table = pass_df.append(fail_df)
            table["sample_id"] = name
            
        else:
            df = pass_df.append(fail_df)
            df["sample_id"] = name
            table = table.append(df)

    table_txt = outfile.replace(".load", ".txt")
    table.to_csv(table_txt, sep="\t", header=True, index=False)

    P.load(table_txt, outfile)
            

@follows(loadflagstatBam)
@transform("bowtie2.dir/*.bam",
           regex(r"(\S+)\.(.*).bam"),
           r"\1_\2.picardAlignmentStats.txt")
def picardAlignmentSummary(infile, outfile):
    '''get aligntment summary stats with picard'''

    tmp_dir = "$SCRATCH_DIR"
    refSeq = os.path.join(PARAMS["annotations_genome_dir"], PARAMS["genome"] + ".fasta")

    job_threads = "4"
    job_memory = "5G"
    
    statement = '''tmp=`mktemp -p %(tmp_dir)s`; checkpoint ;
                   CollectAlignmentSummaryMetrics
                     R=%(refSeq)s
                     I=%(infile)s
                     O=$tmp; checkpoint ;
                   cat $tmp | grep -v "#" > %(outfile)s'''

    print statement

    P.run()


@merge(picardAlignmentSummary,
       "picardAlignmentSummary.load")
def loadpicardAlignmentSummary(infiles, outfile):
    '''load the complexity metrics to a single table in the db'''

    P.concatenateAndLoad(infiles, outfile,
                         regex_filename=".*/(.*).picardAlignmentStats",
                         cat="sample_id",
                         options='-i "sample_id"')


@active_if(Unpaired == False)
@follows(flagstatBam)
@transform("bowtie2.dir/*.bam",
           regex(r"(\S+)\.(.*).bam"),
           r"\1_\2.picardInsertSizeMetrics.txt")
def picardInsertSizes(infile, outfile):
    '''get aligntment summary stats with picard'''

    tmp_dir = "$SCRATCH_DIR"
    
    job_threads = "10"
    job_memory = "40G"

    pdf = outfile.replace("Metrics.txt", "Histogram.pdf")
    histogram = outfile.replace("Metrics.txt", "Histogram.txt")
    
    statement = '''tmp=`mktemp -p %(tmp_dir)s`; checkpoint ;
                   CollectInsertSizeMetrics
                     I=%(infile)s 
                     O=$tmp
                     H=%(pdf)s
                     M=0.5; checkpoint ;
                   cat $tmp | grep -A`wc -l $tmp | tr "[[:blank:]]" "\\n" | head -n 1` "## HISTOGRAM" $tmp | grep -v "#" > %(histogram)s; checkpoint;
                   cat $tmp | grep -A 2 "## METRICS CLASS" $tmp | grep -v "#" > %(outfile)s; checkpoint;
                   rm $tmp'''

    print statement

    P.run()

    
@merge(picardInsertSizes,
       "picardInsertSizeMetrics.load")
def loadpicardInsertSizeMetrics(infiles, outfile):
    '''load the insert size metrics to a single table in the db'''

    P.concatenateAndLoad(infiles, outfile,
                         regex_filename=".*/(.*).picardInsertSizeMetrics",
                         cat="sample_id",
                         options='-i "sample_id"')


@follows(picardInsertSizes)
@merge("bowtie2.dir/*.picardInsertSizeHistogram.txt",
       "picardInsertSizeHistogram.load")
def loadpicardInsertSizeHistogram(infiles, outfile):
    '''load the insert size metrics to a single table in the db'''

    P.concatenateAndLoad(infiles, outfile,
                         regex_filename=".*/(.*).picardInsertSizeHistogram",
                         cat="sample_id",
                         options='-i "sample_id"')

    
@follows(loadpicardAlignmentSummary, loadpicardInsertSizeMetrics, loadpicardInsertSizeHistogram)
def mapping():
    pass


####################################################
#####              Peakcalling                 #####
####################################################
@active_if(Unpaired)
@follows(mapping, mkdir("macs2.dir"))
@transform("bowtie2.dir/*.prep.bam",
           regex("bowtie2.dir/(.*).prep.bam"),
           r"macs2.dir/\1.macs2.fragment_size.tsv")
def macs2Predictd(infile, outfile):
    '''predict fragment sizes for SE ChIP'''

    job_threads = "4"
    
    options = PARAMS["macs2_se_options"]
    outdir = os.path.dirname(outfile)

    statement = '''macs2 predictd 
                     --format BAM 
                     --ifile %(infile)s 
                     --outdir %(outdir)s 
                     --verbose 2 %(options)s 
                     2> %(outfile)s'''

    P.run()

    
@active_if(Unpaired)
@transform(macs2Predictd, suffix(r".fragment_size.tsv"), r".fragment_size.txt")
def getFragmentSize(infile, outfile):
    '''Get fragment sizes from macs2 predictd'''

    sample = os.path.basename(infile).rstrip(".fragment_size.tsv")
    tmp_dir = "$SCRATCH_DIR"
    
    statement = '''tmp=`mktemp -p %(tmp_dir)s`; checkpoint;
                   echo %(sample)s `cat %(infile)s | 
                     grep "# tag size =" | 
                     tr -s " " "\\t" | 
                     awk 'BEGIN {OFS="\\t "} {print $12}'` > $tmp; checkpoint; 
                   cat $tmp | tr -s " " "\\t" > %(outfile)s; checkpoint;
                   rm $tmp'''

    print statement

    P.run()

    
@active_if(Unpaired)
@transform(getFragmentSize, suffix(r".txt"), r".load")
def loadgetFragmentSize(infile, outfile):
    P.load(infile, outfile, options='-H "sample, tag_size"')


@follows(loadgetFragmentSize)
@transform("bowtie2.dir/*.prep.bam",
           regex("bowtie2.dir/(.*).prep.bam"),
           r"macs2.dir/\1.macs2.log")
def macs2callpeaks(infile, outfile):
    '''call peaks with macs2'''

    if BamTools.isPaired(infile):

        options = PARAMS["macs2_pe_options"]
        name = os.path.basename(outfile).rstrip(".macs2.log")
        
        statement='''macs2 callpeak 
                       --outdir macs2.dir                  
                       --bdg
                       --SPMR
                       %(options)s 
                       --treatment %(infile)s 
                       --name %(name)s 
                       >& %(outfile)s'''  

    else:
        # get macs2 predictd fragment lengths from csvdb
        "macs2.dir/\1.macs2.fragment_size.txt"
        table = os.path.basename(infile).replace(".fastq.1.gz", ".macs2.fragment_size").replace("-", "_").replace(".", "_")
        # table = os.path.basename(fragment_size).rstrip(".txt").replace("-", "_").replace(".", "_")

        query = '''select tag_size from %(table)s ''' % locals()

        dbh = sqlite3.connect(PARAMS["database"])
        cc = dbh.cursor()
        sqlresult = cc.execute(query).fetchall()

        fragment_length = sqlresult[0]
        fragment_length = fragment_length[0]

        # run macs2 callpeak
        job_threads = "5"

        options = PARAMS["macs2_se_options"]
        name = os.path.basename(outfile).split(".")[0]
        tmp_dir = "$SCRATCH_DIR"

        statement='''macs2 callpeak 
                       --outdir macs2.dir 
                       --bdg
                       --SPMR
                       %(options)s 
                       --treatment %(infile)s 
                       --name %(name)s 
                       >& %(outfile)s'''  

    print statement
    
    P.run()

    
@follows(macs2callpeaks)
@files(None, "blacklist_chip.mm10.bed.gz")
def getChIPblacklist(infile, outfile):
    '''Get Ensembl ChIP blacklisted regions'''

    chip_blacklist = PARAMS["peak_filter_chip_blacklist"]
    statement = '''wget -O %(outfile)s %(chip_blacklist)s'''

    P.run()

    
@follows(getChIPblacklist)
@files(None, "blacklist_atac.mm10.bed.gz")
def getATACblacklist(infile, outfile):
    '''Get ATAC blacklist regions'''

    atac_blacklist = PARAMS["peak_filter_atac_blacklist"]
    statement = '''wget -q %(atac_blacklist)s | gzip - > %(outfile)s'''

    P.run()

    
@follows(getATACblacklist)
@transform("macs2.dir/*.narrowPeak",
           regex("(.*).narrowPeak"),
           add_inputs([getChIPblacklist, getATACblacklist]),
           r"\1.filt.bed")
def filterPeaks(infiles, outfile):
    '''subtract blacklist regions from peakset'''

    peak, blacklists = infiles

    blacklist = ' '.join(blacklists)
    
    statement = '''intersectBed -wa -v -a %(peak)s -b <(zcat %(blacklist)s ) > %(outfile)s'''

    print statement
    
    P.run()


def mergeReplicatePeaksGenerator():
    '''Get replicate info from pipeline.ini & create jobs'''
    
    replicates = PARAMS["replicates_pairs"]    

    if len(replicates)==0:
        pass

    outDir = "macs2.dir/"
    
    # if PARAMS["macs2_peaks"]=="all":
    #     suffix = "_peaks.filt.bed"
    
    # if PARAMS["macs2_peaks"]=="size_filt":
    #     suffix = ".size_filt_peaks.filt.bed"

    peaksets = [".all", ".size_filt"]

    for peaks in peaksets:
        if peaks == ".all":
            suffix = "_peaks.filt.bed"
        if peaks == ".size_filt":
            suffix = ".size_filt_peaks.filt.bed"
        
        for reps in replicates.split('\n'):
            reps = reps.split(",")
            out = outDir + reps[2] + "_merged" + peaks + ".bed"
            bed1 = outDir + reps[0] + suffix
            bed2 = outDir + reps[1] + suffix

            yield [ [bed1, bed2], out]

        
@follows(filterPeaks)
@active_if(PARAMS["replicates_merge_reps"])
@files(mergeReplicatePeaksGenerator)
def mergeReplicatePeaks(infiles, outfile):
    '''Merge replicate peaks'''

    pair1, pair2 = infiles

    beda = BedTool(pair1)
    bedb = BedTool(pair2)

    tmp = outfile.replace(".bed", ".tmp")
    
    ab_bed = beda.intersect(bedb, wa=True, wb=True)
    ab_bed.saveas(tmp)

    with open(outfile, "w") as output:
        n = 0
        with open(tmp, "r") as o:
            for line in o:
                n = n + 1
                fields = [x.strip('"\n') for x in line.split("\t")]

                # destructure list, assign items to variables
                [chr1, start1, end1, peak_id1, peak_width1, strand1, fold_change1, pvalue1, qvalue1, summit1, 
                chr2, start2, end2, peak_id2, peak_width2, strand2, fold_change2, pvalue2, qvalue2, summit2] = fields

                # merge columns between reps as appropriate
                start = min(start1, end1, start2, end2)
                end = max(start1, end1, start2, end2)
                peak_width = str(int(end) - int(start))
                peak_id = '.'.join([peak_id1, peak_id2])

        #         strand = [strand1 if strand1 == strand2 else "."]
        #         strand = sum(strand, [])
                if strand1 == strand2:
                    strand = strand1
                else:
                    strand = "."

                fold_change = str(np.mean([float(fold_change1), float(fold_change2)]))
                pvalue = min(pvalue1, pvalue2)
                qvalue = min(qvalue1, qvalue2)
                peak_center = str(int(start) + (int(peak_width)/2))

                # output
                bed = [chr1, start, end, peak_id, peak_width, strand, fold_change, pvalue, qvalue, peak_center]
                bed = '\t'.join(bed) + '\n'

                if n == 1:
                    output.write('\t'.join(["contig", "start", "end", "peak_id", "peak_width", "strand", "fold_change", 
                                            "pvalue", "qvalue", "summit"]) + '\n')
                    output.write(bed)
                else:
                    output.write(bed)

    statement = '''rm %(tmp)s'''

    P.run()


@follows(mergeReplicatePeaks)
@files(None, "macs2.dir/no_peaks.txt")
def countPeaks(infiles, outfile):

    beds = glob.glob("/gfs/work/tkhoyratty/AirPouch_ATAC/analysis/atac_pipeline_trim/macs2.dir/*filt.bed")
    merge_beds = glob.glob("/gfs/work/tkhoyratty/AirPouch_ATAC/analysis/atac_pipeline_trim/macs2.dir/*merged*.bed")

    peaksets = [beds, merge_beds]

    if len(peaksets)==0:
        pass
    
    no_peaks = {}

    for peaks in peaksets:
        for bed in peaks:
            if "merged" in bed:
                name = '.'.join(os.path.basename(bed).split(".")[0:2]).rstrip(".all")
            else:
                name = os.path.basename(bed).replace("_peaks.filt.bed", "")

            df = pd.read_csv(bed, sep="\t", header=None)

            no_peaks[name] = len(df)

    peaks = pd.DataFrame.from_dict(no_peaks, orient="index")

    peaks["sample_id"] = peaks.index.values
    peaks["size_filt"] = peaks["sample_id"].apply(lambda x: "all_fragments" if "size_filt" not in x else "<150bp")
    peaks["merged"] = peaks.apply(lambda x: "merged" if "merged" in x.sample_id else "replicate", axis=1)
    peaks["sample_id"] = peaks["sample_id"].apply(lambda x: x.split(".")[0].rstrip("_merged"))
    peaks = peaks.rename(columns={0:"no_peaks"})
    peaks.reset_index(inplace=True, drop=True)

    peaks.to_csv(outfile, header=True, index=False, sep="\t")

#     beds = glob.glob("/gfs/work/tkhoyratty/AirPouch_ATAC/analysis/atac_pipeline_trim/macs2.dir/*filt.bed")

#     # print beds
#     n = 0
#     no_peaks = {}

#     for bed in beds:
#         n = n + 1
#         name = os.path.basename(bed).replace("_peaks.filt.bed", "")
#         df = pd.read_csv(bed, sep="\t", header=None)

#         if n==1:
#             no_peaks = {name : len(df)}
#         else:
#             no_peaks[name] = len(df)

#     peaks = pd.DataFrame.from_dict(no_peaks, orient="index")

#     peaks["sample_id"] = peaks.index.values
#     peaks["size_filt"] = peaks["sample_id"].apply(lambda x: "all_fragments" if "size_filt" not in x else "<150bp")
#     peaks["sample_id"] = peaks["sample_id"].apply(lambda x: x.split(".")[0])
#     peaks = peaks.rename(columns={0:"no_peaks"})
#     peaks.reset_index(inplace=True, drop=True)

#     peaks.to_csv(outfile, header=True, index=False, sep="\t")

    
@transform(countPeaks, suffix(".txt"), ".load")
def loadcountPeaks(infile, outfile):
    P.load(infile, outfile, options='-i "sample_id" ')

    
@follows(loadcountPeaks)
def peakcalling():
    pass


########################################################
####                    FRIP                        ####
########################################################
def generate_FRIPcountBAM_jobs():

    all_intervals = glob.glob("macs2.dir/*_peaks.narrowPeak")
    all_bams = glob.glob("bowtie2.dir/*.prep.bam")
        
    outDir = "FRIP.dir/"

    # group size filtered and non-size filtered files seperately
    bams = [x for x in all_bams if "size_filt" in x]
    intervals = [x for x in all_intervals if "size_filt" in x]
    size_filt = [bams, intervals]
    
    bams = [x for x in all_bams if "size_filt" not in x]
    intervals = [x for x in all_intervals if "size_filt" not in x]
    non_size_filt = [intervals, bams]

    # iterate over grouped files matching bams & peaks
    for group in [size_filt, non_size_filt]:
        group = sum(group, []) # first flatten list

        intervals = [x for x in group if "narrowPeak" in x]
        bams = [x for x in group if "prep.bam" in x]

        for interval in intervals:
            ifile = [i.split("/")[-1].rstrip("_peaks.narrowPeak") for i in [interval] ]
            match = ''.join(ifile)

            for bam in bams:
                bam_sample = bam.split("/")[-1].rstrip(".prep.bam")

                if match in bam and match in interval:
                    bfile = [b.split("/")[-1][:-len(".prep.bam")] for b in [bam] ]

                    bedfile = ' '.join(str(x) for x in ifile )
                    bamfile = ' '.join(str(x) for x in bfile )

                    output = outDir + bedfile + "_fripcounts" + ".txt"

                    a = os.path.basename(interval)[:-len("_peaks.narrowPeak")]
                    b = os.path.basename(bam)[:-len(".prep.bam")]
                    c = os.path.basename(output)[:-len("_fripcounts.txt")]

                    if a == b and b == c: #sanity check
                        yield ( [ [interval, bam], output ] )

                        
@follows(mkdir("FRIP.dir"), peakcalling)
@files(generate_FRIPcountBAM_jobs)
def FRIPcountBAM(infiles, outfile):
    '''use bedtools to count reads in bed intervals'''

    interval, bam = infiles

    if BamTools.isPaired(bam):
         # -p flag specifes only to count paired reads

        statement = '''bedtools multicov -p -q 10 -bams %(bam)s 
                    -bed <( awk 'BEGIN {OFS="\\t"} {print $1,$2,$3,$4,$5,$3-$2}' %(interval)s ) 
                    > %(outfile)s 
                    && sed -i '1i \chromosome\\tstart\\tend\\tpeak_id\\tpeak_score\\tpeak_width\\ttotal' 
                    %(outfile)s''' % locals()

    else:

         statement = '''bedtools multicov -q 10 -bams %(bam)s 
                     -bed <( awk 'BEGIN {OFS="\\t"} {print $1,$2,$3,$4,$5,$3-$2}' %(interval)s ) 
                     > %(outfile)s 
                     && sed -i '1i \chromosome\\tstart\\tend\\tpeak_id\\tpeak_score\\tpeak_width\\ttotal' 
                     %(outfile)s''' % locals()

    print statement
         
    P.run()

    
@merge(FRIPcountBAM, "FRIP.dir/frip_table.txt")
def FRIP(infiles, outfile):
    '''Calculate fraction of read in peaks'''

    for infile in infiles:
        frip_tsv = infile.replace("_fripcounts.txt", "_frip.txt")

        counts = pd.read_csv(infile, sep="\t", header=0)
        peak_counts = float(np.sum(counts["total"]))

        if "size_filt" in infile:
            name = os.path.basename(infile).replace("_fripcounts.txt", "").replace(".", "_")
        else:
            name = os.path.basename(infile).replace("_fripcounts.txt", "_prep")

        db = PARAMS["general_database"]

        if Unpaired == False:
            query = '''select properly_paired/2 as total from flagstats where QC_status = "pass" and sample_id = "%(name)s" ''' % locals()

        else:
            query = '''select mapped as total from flagstats where QC_status = "pass" and sample_id = "%(name)s" ''' % locals()

        total_counts = DB.fetch_DataFrame(query, db)
        total_counts = total_counts["total"]

        FRIP = peak_counts/total_counts

        table_name = name.rstrip("_prep")
        dict_FRIP = {table_name:FRIP}
        df = pd.DataFrame.from_dict(dict_FRIP, orient = "index")

        # format df
        df["sample_id"] = df.index.values
        df.columns = ["FRIP", "sample_id"]
        df["size_filt"] = df.apply(lambda x: "<150bp" if "size_filt" in x.sample_id else "all_fragments", axis=1)
        df["sample_id"] = df["sample_id"].apply(lambda x: x.strip("_size_filt"))

        df.to_csv(frip_tsv, sep="\t", header=True, index=False)

    # merge all files to load to csvdb
    files = glob.glob("FRIP.dir/*_frip.txt")
    head = files[0]
    if len(files)==0:
        pass
    files = ' '.join(files)

    tmp_dir = "$SCRATCH_DIR"
    
    statement = '''tmp=`mktemp -p %(tmp_dir)s`; checkpoint;
                   head -n1 %(head)s > $tmp; checkpoint;
                   for i in %(files)s; do tail -n +2 $i >> $tmp; done; checkpoint;
                   mv $tmp %(outfile)s'''

    P.run()


@transform(FRIP, suffix(".txt"), ".load")
def loadFRIP(infile, outfile):
    P.load(infile, outfile, options = '-i "sample_id"')


@follows(loadFRIP)
def frip():
    pass

    
########################################################
####                  Merge Peaks                   ####
########################################################
@follows(mkdir("BAM_counts.dir"), frip)
@merge("macs2.dir/*_peaks.filt.bed", "BAM_counts.dir/merged_peaks.bed")
def mergePeaks(infiles, outfile):
    '''cat all peak files, center over peak summit +/- 250 b.p., then merge peaks'''

    tmp_dir = "$SCRATCH_DIR"
    window_size = PARAMS["read_counts_window"]
    offset = int(window_size)/2

    # defualt is to use non size filtered peaks
    if PARAMS["macs2_peaks"] == "all":
        infiles = [x for x in infiles if "size_filt" not in x]
    if PARAMS["macs2_peaks"] == "size_filt":
        infiles = [x for x in infiles if "size_filt" in x]

    infiles = ' '.join(infiles)

    statement = '''tmp=`mktemp -p %(tmp_dir)s`; checkpoint;
                   cat %(infiles)s | grep -v ^chrUn* -  > $tmp; checkpoint;
                   awk 'BEGIN {OFS="\\t"} {center = $2 + $10 ; start = center - %(offset)s ; end = center + %(offset)s ;
                     print $1,start,end,$4,$5}' $tmp | 
                   awk 'BEGIN {OFS="\\t"} {if ($2 < $3) print $0}' - |
                   sort -k1,1 -k2,2n |
                   mergeBed -c 4,5 -o count,mean -i - |
                   awk 'BEGIN {OFS="\\t"} {print $1,$2,$3,"merged_peaks_"NR,$5,$4,$3-$2,sprintf("%%i", ($2+$3)/2)}' - > %(outfile)s; checkpoint;
                   rm $tmp'''

    print statement

    P.run()

    
########################################################
####                GREAT Peak2Gene                 ####
########################################################
@follows(mergePeaks, mkdir("annotations.dir"))
@files(None,"annotations.dir/ensemblGeneset.txt")
def fetchEnsemblGeneset(infile,outfile):
    ''' Get the *latest* gene records using biomart. The aim here is NOT to match
        the great gene set: For that we would only want protein coding genes with
        GO annotations '''

    statement = '''select gi.gene_id, gi.gene_name,
                          gs.contig, gs.start, gs.end, gs.strand
                   from gene_info gi
                   inner join gene_stats gs
                   on gi.gene_id=gs.gene_id
                   where gi.gene_biotype="protein_coding"
                '''

    anndb = os.path.join(PARAMS["annotations_dir"],
                         "csvdb")

    df = DB.fetch_DataFrame(statement, anndb)
    df.to_csv(outfile, index=False, sep="\t", header=True)

    
@transform(fetchEnsemblGeneset,suffix(".txt"),".load")
def uploadEnsGenes(infile,outfile):
    '''Load the ensembl annotation including placeholder GO ID's'''
    P.load(infile, outfile, options='-i "gene_id" -i "go_id" ')

    
@follows(uploadEnsGenes)
def getGeneLists():
    pass


@follows(getGeneLists, mkdir("greatBeds.dir"))
@files(uploadEnsGenes, "greatBeds.dir/ens_great_prom.bed")
def greatPromoters(infile,outfile):
    ''' Make great promoters for the genes retrieved from Ensembl'''

    dbh = sqlite3.connect(PARAMS["database"])
    cc = dbh.cursor()

    basalup = PARAMS["great_basal_up"]
    basaldown = PARAMS["great_basal_down"]
    maxext = PARAMS["great_max"]
    half = PARAMS["great_half"]  
    statement = '''select distinct contig, start,                                                                                                         
                   end, strand, gene_id from ensemblGeneset '''

    result = cc.execute(statement).fetchall()
    
    
    locations = [ [ str(r[0]), int(r[1]), int(r[2]),str(r[3]), str(r[4]) ] 
                   for r in result ]
    
    writeGreat(locations,basalup,basaldown,maxext,outfile,half)


@transform(greatPromoters,
           regex(r"(.*)_prom.bed"),
           r"\1.bed")
def filterEnsPromoters(infile,outfile):
    '''Remove unwanted chromosomes & correct contig column, "chrchr" -> "chr"'''

    tmp_dir = "$SCRATCH_DIR"
    statement = '''tmp=`mktemp -p %(tmp_dir)s`; 
                sed 's/chrchr/chr/' %(infile)s > $tmp &&
                grep -v ^chrM $tmp > %(outfile)s && rm $tmp'''

    print statement
    
    P.run()

    
@transform(filterEnsPromoters,suffix(".bed"),".load")
def loadGreatPromoters(infile, outfile):
    '''Load the great promoter regions'''
    P.load(infile, outfile, options='-H "chr,start,end,gene_id" -i "gene_id"')

    
@follows(loadGreatPromoters)
def GreatAnnotation():
    pass


@follows(GreatAnnotation, mkdir("regulated_genes.dir"))
@transform("BAM_counts.dir/merged_peaks.bed",
           regex(r"BAM_counts.dir/(.*).bed"),
           add_inputs("greatBeds.dir/ens_great.bed"),
           r"regulated_genes.dir/\1.GREAT.txt")
def regulatedGenes(infiles,outfile):
    '''Get all genes associated with peaks'''

    infile, greatPromoters = infiles

    # intersect infiles with great gene annotation beds to get peak associated genes
    statement = '''intersectBed -wa -wb -a <(cut -f1-8 %(infile)s) -b %(greatPromoters)s 
                | cut -f1-8,12 > %(outfile)s''' % locals()

    # Filter on nearest peak 2 gene later

    print statement

    P.run()

    
@transform(regulatedGenes, suffix(r".txt"), r".load")
def loadRegulatedGenes(infile, outfile):
    P.load(infile, outfile, 
           options='-H "contig,start,end,peak_id,peak_score,no_peaks,peak_width,peak_centre,gene_id" -i "peak_id"')

    
@transform(loadRegulatedGenes,
           suffix(r".load"),
           add_inputs(loadGreatPromoters,uploadEnsGenes),
           r".annotated.bed")
def regulatedTables(infiles, outfile):
    '''Make an informative table about peaks and "regulated" genes'''

    regulated, great, ensGenes = [ P.toTable(x) for x in infiles ]

    query = '''select distinct r.contig,
                  r.start, r.end, r.peak_id, r.peak_score,
                  r.no_peaks, r.peak_width, r.peak_centre,
                  g.gene_id, e.gene_name, e.strand,
                  e.start, e.end
                  from %s as r
                  inner join %s as g
                     on g.gene_id = r.gene_id 
                  inner join %s as e
                     on g.gene_id = e.gene_id
                  ''' % (regulated, great, ensGenes)

    print query
    
    dbh = sqlite3.connect(PARAMS["database"])
    cc = dbh.cursor()
    sqlresult = cc.execute(query).fetchall()

    sql_table = outfile.replace(".bed", ".txt")
    
    o = open(sql_table,"w")
    o.write("\t".join ( 
            ["chromosome","peak_start","peak_end","peak_id","peak_score",
             "no_peaks","peak_width","peak_centre",
             "dist2peak","gene_id", "TSS"]) + "\n" )

    for r in sqlresult:
        contig, pstart, pend, peak_id, peak_score, no_peaks, peak_width, peak_centre = r[0:8]
        gene_id, gene_name, gene_strand, gene_start, gene_end = r[8:14]
        
        if gene_strand == "+": gstrand = 1
        else: gstrand = 2

        tss = getTSS(gene_start,gene_end,gene_strand)

        pwidth = max(pstart,pend) - min(pstart,pend)
        ploc = (pstart + pend)/2

        if gstrand==1: tssdist = tss - ploc
        else: tssdist = ploc - tss

        columns = [ str(x) for x in
                    [  contig, pstart, pend, peak_id, peak_score, peak_width, peak_centre, tssdist, gene_id, tss] ]
        o.write("\t".join( columns  ) + "\n")
    o.close()

    # get closest genes 2 peaks, 1 gene per peak
    tmp_dir = "$SCRATCH_DIR"
    statement = '''tmp=`mktemp -p %(tmp_dir)s`;
                tail -n +2 %(sql_table)s  | sed 's/-//g' 
                | awk 'BEGIN {OFS="\\t"} {print $4,$5,$6,$7,$8,$9,$10,$1,$2,$3}' 
                | sort -k8,8 -k9,9n -k7,7n 
                | cat | uniq -f7 > $tmp 
                && awk 'BEGIN {OFS="\\t"} {print $8,$9,$10,$1,$2,$3,$4,$5,$6,$7}' $tmp 
                > %(outfile)s  && rm %(sql_table)s $tmp''' % locals()

    print statement

    P.run()

    
@transform(regulatedTables, suffix(".bed"), ".load")
def loadRegulatedTables(infile,outfile):
    P.load(infile,outfile,
           options='-H"contig,peak_start,peak_end,peak_id,peak_score,peak_width,peak_centre,TSSdist,gene_id,TSS" -i "peak_id" ')


@follows(loadRegulatedTables)
def great():
    pass


########################################################
####     Differential Accessibility Read Counts     ####
########################################################
@follows(great)
@transform("bowtie2.dir/*.prep.bam",
           regex(r"(.*).prep.bam"),
           r"\1.prep.bam.bai")
def indexBAM(infile, outfile):
    '''Index input BAM files'''

    statement = '''samtools index %(infile)s %(outfile)s'''

    P.run()

    
def generate_scoreIntervalsBAM_jobs():
    
    # list of bed files & bam files, from which to create jobs
    intervals = glob.glob("regulated_genes.dir/*annotated.bed")
    bams = glob.glob("bowtie2.dir/*.prep.bam")

    outDir = "BAM_counts.dir/"

    for interval in intervals:
        #print interval
        ifile = [i.split("/")[-1][:-len(".annotated.bed")] for i in [interval] ]
        # iterate over intervals generating infiles & partial filenames

        for bam in bams:
            bfile = [b.split("/")[-1][:-len(".prep.bam")] for b in [bam] ]
            # for each interval, iterate over bams generating infiles & partial filenames
            bedfile = ' '.join(str(x) for x in ifile )
            bamfile = ' '.join(str(x) for x in bfile )

            output = outDir + bedfile + "." + bamfile + "_counts.txt"
            # output = outfiles. 1 for each bed/bam combination

            yield ( [ [interval, bam], output ] )

            
@follows(indexBAM)
@files(generate_scoreIntervalsBAM_jobs)
def scoreIntervalsBAM(infiles, outfile):
    '''use bedtools to count reads in bed intervals'''

    interval, bam = infiles

    tmp_dir = "$SCRATCH_DIR"
    
    if BamTools.isPaired(bam):
        statement = '''tmp=`mktemp -p %(tmp_dir)s`;
                       cut -f1-7,9 %(interval)s > $tmp; checkpoint;
                       bedtools multicov -p -q 10 -bams %(bam)s -bed $tmp > %(outfile)s; checkpoint;
                       sed -i '1i \chromosome\\tstart\\tend\\tpeak_id\\tpeak_score\\tpeak_width\\tpeak_center\\tgene_id\\ttotal' %(outfile)s; checkpoint;
                       rm $tmp''' % locals()
        
    else:
         statement = '''tmp=`mktemp -p %(tmp_dir)s`;
                        cut -f1-7,9 %(interval)s > $tmp; checkpoint;
                        bedtools multicov -q 10 -bams %(bam)s -bed $tmp > %(outfile)s; checkpoint; 
                        sed -i '1i \chromosome\\tstart\\tend\\tpeak_id\\tpeak_score\\tpeak_width\\tpeak_center\\tgene_id\\ttotal' %(outfile)s; checkpoint;
                        rm $tmp''' % locals()

    print statement

    P.run()

    
@transform(scoreIntervalsBAM,
           regex(r"(.*).counts.txt"),
           r"\1_norm_counts.txt")
def normaliseBAMcounts(infile, outfile):
    '''normalise BAM counts for file size'''
       
    counts = pd.read_csv(infile, sep="\t", header=0)

    if "size_filt" in infile:
        name = os.path.basename(infile).replace("_counts.txt", "").replace(".", "_").lstrip("merged_peaks_GREAT_")
    else:
        name = os.path.basename(infile).rstrip(".counts.txt").split(".")[-1] + "prep"
        
    db = PARAMS["general_database"]

    if Unpaired == False:
        query = '''select properly_paired/2 as total from flagstats where QC_status = "pass" and sample_id = "%(name)s" ''' % locals()
    else:
        query = '''select mapped as total from flagstats where QC_status = "pass" and sample_id = "%(name)s" ''' % locals()

    total_counts = DB.fetch_DataFrame(query, db)

    norm_factor = float(total_counts["total"])/1000000 # total_counts/1x10^6

    counts["RPM"] = counts["total"].apply(lambda x: x/norm_factor)
    counts["RPM_width_norm"] = counts.apply(lambda x: x.RPM/x.peak_width if x.peak_width > 0 else x.RPM/1, axis=1)
    counts["sample_id"] = name.rstrip("_prep")
    counts["size_filt"] = counts.apply(lambda x: "<150bp" if "size_filt" in x.sample_id else "all_fragments", axis=1)
    counts["sample_id"] = counts["sample_id"].apply(lambda x: x.strip("_size_filt"))
    
    counts.to_csv(outfile, sep="\t", header=True, index=False)

                
@follows(normaliseBAMcounts)
@merge("BAM_counts.dir/*_norm_counts.txt", "all_norm_counts.txt")
def mergeNormCounts(infiles, outfile):

    infiles = [x for x in infiles if "GREAT" in x] # hack, filter preventing inclusion of counts for FRIP if running pipeline out of sequence
    head = infiles[0]
    infiles = ' '.join(infiles)

    tmp_dir = "$SCRATCH_DIR"
    statement = '''tmp=`mktemp -p %(tmp_dir)s`; checkpoint;
                   head -n 1 %(head)s >  $tmp; checkpoint;
                   for i in %(infiles)s; do tail -n +2 $i >> $tmp; done; checkpoint;
                   mv $tmp %(outfile)s'''
    
    P.run()
    

@transform(mergeNormCounts, suffix(r".txt"), r".load")
def loadmergeNormCounts(infile, outfile):
    P.load(infile, outfile, options='-i "peak_id" ')

@follows(loadmergeNormCounts)
def count():
    pass


########################################################
####              Coverage tracks                   ####
########################################################
@follows(count, mkdir("deeptools.dir"))
@transform("bowtie2.dir/*.prep.bam",
           regex(r"(.*).bam"),
           r"\1.bam.bai")
def indexPrepBam(infile, outfile):
    '''samtools index bam'''

    statement = '''samtools index -b %(infile)s %(outfile)s'''

    P.run()

    
@follows(indexPrepBam)
@transform("bowtie2.dir/*.prep.bam",
           regex(r"bowtie2.dir/(.*).prep.bam"),
           r"deeptools.dir/\1.coverage.bw")
def bamCoverage(infile, outfile):
    '''Make normalised bigwig tracks with deeptools'''

    job_memory = "4G"
    job_threads = "10"

    if BamTools.isPaired(infile):

        # PE reads filtered on sam flag 66 -> include only first read in properly mapped pairs
        
        statement = '''bamCoverage -b %(infile)s -o %(outfile)s
                    --binSize 5
                    --smoothLength 20
                    --centerReads
                    --normalizeUsingRPKM
                    --samFlagInclude 66
                    -p "max"'''
        
    else:

        # SE reads filtered on sam flag 4 -> exclude unmapped reads
        
        statement = '''bamCoverage -b %(infile)s -o %(outfile)s
                    --binSize 5
                    --smoothLength 20
                    --centerReads
                    --normalizeUsingRPKM
                    --samFlagExclude 4
                    -p "max"'''

    # added smoothLength = 20 to try and get better looking plots...
    # --minMappingQuality 10 optional argument, but unnecessary as bams are alredy filtered
    # centerReads option and small binsize should increase resolution around enriched areas

    print statement
    
    P.run()


@follows(indexPrepBam)
@transform("bowtie2.dir/*.prep.bam",
           regex(r"bowtie2.dir/(.*).prep.bam"),
           r"deeptools.dir/\1.coverage.sizeFilt.bw")
def bamCoverage_sizeFilter(infile, outfile):
    '''Make normalised bigwig tracks with deeptools'''

    job_memory = "2G"
    job_threads = "10"

    if BamTools.isPaired(infile):
        
        statement = '''bamCoverage -b %(infile)s -o %(outfile)s
                    --binSize 5
                    --maxFragmentLength 150
                    --smoothLength 20
                    --centerReads
                    --normalizeUsingRPKM
                    --samFlagInclude 66
                    -p "max"'''
        
    else:
        
        statement = '''echo "Error - BAM must be PE to use --maxFragmentLength parameter" > %(outfile)'''


    print statement
    
    P.run()

    
@follows(bamCoverage, bamCoverage_sizeFilter)
def coverage():
    pass

########################################################
####                    QC Plots                    ####
########################################################
@follows(coverage, mkdir("QC_plots"))
@files(None, "sample_info.txt")
def getSampleInfo(infile, outfile):
    '''Generate lookup table of sample info base on pipeline.ini'''
    
    sample_info = PARAMS["sample_info"]

    n = 0
    for line in sample_info.split('\n'):
        n = n + 1
        fields = [x.strip(' ') for x in line.split(',')]

        if n ==1:
            df = pd.DataFrame(fields).transpose()
            df.columns = ["sample_id", "condition", "replicate"]
        else:
            df1 =  pd.DataFrame(fields).transpose()
            df1.columns = ["sample_id", "condition", "replicate"]
            df = df.append(df1)

    df["category"] = df.apply(lambda x: str('_'.join([x.condition, x.replicate])), axis=1)
    
    df.to_csv(outfile, sep="\t", header=True, index=False)

    
@transform(getSampleInfo, suffix(".txt"), ".load")
def loadgetSampleInfo(infile, outfile):
    P.load(infile, outfile, options='-i "sample_id" ')


@follows(loadgetSampleInfo)
@files(None, ["QC_plots/mapped_pairs.png",
              "QC_plots/pct_aligned_in_pairs.png",
              "QC_plots/pct_adapter.png",
              "QC_plots/pct_chrM.png",
              "QC_plots/mapping_QC_stats.txt"])
def mappingPlots(infile, outfiles):
    '''Collect all mapping stats & retrun df for plotting'''

    db=PARAMS["database"]
    sample_info = DB.fetch_DataFrame('''select * from sample_info''', db)
    
    if Unpaired==False:
        reads = DB.fetch_DataFrame('''select READS_ALIGNED_IN_PAIRS/2 as MAPPED_PAIRS, PCT_READS_ALIGNED_IN_PAIRS,
                                         TOTAL_READS, PCT_ADAPTER, sample_id from picardAlignmentSummary
                                         where CATEGORY = "PAIR" ''', db)
    if Unpaired==True:
        print "Update function for non-paired data"

    # Format mapping qc df
    reads["Filter"] = reads["sample_id"].apply(lambda x: x.split(".")[-1])
    reads["Filter"] = reads["sample_id"].apply(lambda x: x.split("_")[-1] if "size_filt_prep" not in x else "prep<150bp")
    reads["sample_id"] = reads["sample_id"].apply(lambda x: '_'.join(x.split("_")[0:-1]))
    reads["sample_id"] = reads["sample_id"].apply(lambda x: x.split(".")[0])

    if len(sample_info)==0:
        print "Provide sample_info df with sample annotations"

    reads = pd.merge(reads, sample_info, on="sample_id", how="inner")

    # get no. reads mapping to chrM
    chrm = DB.fetch_DataFrame('''select * from allContig''', db)

    # reformat df
    chrm = chrm.pivot("sample_id", "contig", "mapped_reads")
    chrm["total_mapped_reads"] = chrm.sum(axis=1)
    chrm["sample_id"] = chrm.index.values
    chrm.index.name = None
    chrm = chrm[["chrM", "total_mapped_reads", "sample_id"]]
    chrm["pct_chrM"] = chrm["chrM"] / chrm["total_mapped_reads"] *100 # % reads mapping to chrM

    # annotate df
    chrm["sample_id"] = chrm["sample_id"].apply(lambda x: str(x).split(".")[0])
    chrm = pd.merge(chrm, sample_info, on="sample_id", how="inner")
    chrm["Filter"] = "genome" # chrM only in genomic reads as filtered out after, others not tested

    mapping_qc = pd.merge(reads, chrm[["pct_chrM", "sample_id", "Filter"]], how="outer", on=["sample_id", "Filter"])
    mapping_qc.to_csv(outfiles[4], sep="\t", header=True, index=False)
    
    sns.set(style="whitegrid", palette="muted") # set seaborn theme

    # make plots
    sns_total_reads = sns.factorplot(data=mapping_qc, y="MAPPED_PAIRS", x="category", hue="Filter", kind="bar", size=4, aspect=2)
    sns_total_reads.savefig(outfiles[0])
    
    sns_mapped_pairs = sns.factorplot(data=mapping_qc, y="PCT_READS_ALIGNED_IN_PAIRS", x="category", hue="Filter", kind="bar", size=4, aspect=2)
    sns_mapped_pairs.savefig(outfiles[1])
    
    sns_adaptor = sns.factorplot(data=mapping_qc, y="PCT_ADAPTER", x="category", hue="Filter", kind="bar", size=4, aspect=2)
    sns_adaptor.savefig(outfiles[2])
    
    sns_chrm = sns.factorplot(data=mapping_qc, y="pct_chrM", x="category", hue="Filter", kind="bar", size=4, aspect=2)
    sns_chrm.savefig(outfiles[3])


# @follows(mappingPlots)
# @transform("QC_plots/mapping_QC_stats.txt", suffix(".txt"), ".load")
# def loadmappingPlots(infile, outfile):
#     P.load(infile, outfile, options='-i "sample_id" ')


@follows(loadgetSampleInfo)
@files(None, ["QC_plots/fragment_box.png",
              "QC_plots/fragment_hist_all.png",
              "QC_plots/fragment_hist_150.png",
              "QC_plots/insert_sizes.txt"])
def insertSizePlots(infile, outfiles):
    '''Collect insert size metrics & generate plots'''

    db = PARAMS["database"]
    sample_info = DB.fetch_DataFrame('''select * from sample_info''', db)

    def clean(df):
        df["Filter"] = df["sample_id"].apply(lambda x: x.split(".")[-1])
        df["Filter"] = df["sample_id"].apply(lambda x: x.split("_")[-1] if "size_filt_prep" not in x else "prep<150bp")
        df["sample_id"] = df["sample_id"].apply(lambda x: x.rstrip("_prep"))
        df["sample_id"] = df["sample_id"].apply(lambda x: x.split(".")[0])
        df = pd.merge(df, sample_info, on="sample_id", how="inner")
        return df

    insert_sizes = DB.fetch_DataFrame('''select * from picardInsertSizeHistogram where sample_id like "%prep"''', db)
    insert_sizes = clean(insert_sizes)
    insert_sizes.to_csv(outfiles[3], sep="\t", header=True, index=False)   

    # plots
    sns.set(style="whitegrid", palette="muted")
    
    sns_fragment_box = sns.factorplot(data=insert_sizes, x="category", y="insert_size", hue="Filter", kind="box", size=4, aspect=2)
    sns_fragment_box.savefig(outfiles[0])

    sns_fragment_hist_all = sns.factorplot(data=insert_sizes[insert_sizes["Filter"]=="prep"], ci=None,
                                           x="insert_size", y="All_Reads_fr_count", hue="Filter", kind="bar", size=4, aspect=2)
    sns_fragment_hist_all.set(xticks=range(50, 1000, 50), xticklabels=range(50, 1000, 50))
    sns_fragment_hist_all.set_xticklabels(rotation=30)
    
    sns_fragment_hist_all.savefig(outfiles[1])

    sns_fragment_hist_size_filt = sns.factorplot(data=insert_sizes[insert_sizes["Filter"]=="prep<150bp"], ci=None,
                                                 x="insert_size", y="All_Reads_fr_count", hue="Filter", kind="bar", size=4, aspect=2)
    sns_fragment_hist_size_filt.set(xticks=range(10, 150, 10), xticklabels=range(10, 150, 10))
    sns_fragment_hist_size_filt.set_xticklabels(rotation=30)
    
    sns_fragment_hist_size_filt.savefig(outfiles[2])

    
@follows(loadgetSampleInfo)
@files(None, ["QC_plots/no_peaks.png",
              "QC_plots/frip.png",
              "QC_plots/peak_stats.txt"])
def peakCallingPlots(infile, outfiles):
    '''Collect peak calling info & generate plots'''

    db = PARAMS["database"]
    sample_info = DB.fetch_DataFrame('''select * from sample_info''', db)

    peak_stats = DB.fetch_DataFrame('''select a.no_peaks, a.size_filt, b.FRIP, b.sample_id from no_peaks a, 
                                    frip_table b where a.sample_id=b.sample_id and a.size_filt=b.size_filt ''', db)

    peak_stats = pd.merge(peak_stats, sample_info, how="inner", on="sample_id")
    peak_stats.to_csv(outfiles[2], sep="\t", header=True, index=False)
    
    sns.set(style="whitegrid", palette="muted")
    sns_no_peaks = sns.factorplot(data=peak_stats, y="no_peaks", x="category", hue="size_filt", kind="bar", size=4, aspect=2)
    sns_no_peaks.savefig(outfiles[0])
    
    sns_frip = sns.factorplot(data=peak_stats, y="FRIP", x="category", hue="size_filt", kind="bar", size=4, aspect=2)
    sns_frip.savefig(outfiles[1])


@follows(loadgetSampleInfo)
@files(None, ["QC_plots/merged_peaksets.png",
              "QC_plots/merged_peaksets.txt"])
def mergedPeaksetPlots(infile, outfiles):
    '''Plot number of peaks in merged peaksets'''

    db = PARAMS["database"]
    
    merged_peaks = DB.fetch_DataFrame('''select * from no_peaks where merged like "%merged" ''', db)
    merged_peaks.to_csv(outfiles[1], sep="\t", header=True, index=False)

    sns.set(style="whitegrid", palette="muted")    
    sns_merged_peakset = sns.factorplot(data=merged_peaks, kind="bar", y="no_peaks", x="sample_id", hue="size_filt", size=4, aspect=2)
    sns_merged_peakset.savefig(outfiles[0])


@follows(loadgetSampleInfo)
@files(None, ["QC_plots/pearsonRepCorr_*png"])
def replicate_correlation(infile, outfiles):
    '''Plot replicate correlation for counts in merged peakset'''

    db = PARAMS["database"]
    sample_info = DB.fetch_DataFrame('''select * from sample_info''', db)


    # use reads < 150bp or all reads
    if PARAMS["macs2_peaks"] == "all":
        size_filt = "all_fragments"
    if PARAMS["macs2_peaks"] == "size_filt":
        size_filt = "<150bp"
    
    statement = '''select sample_id, peak_id, RPM_width_norm *1000 as RPM 
                   from all_norm_counts where size_filt == "%(size_filt)s" ''' % locals()
    print statement
    
    def get_counts(statement):
        # get df, filter on fragment size

        df = DB.fetch_DataFrame(statement, db)

        df = df.pivot("sample_id", "peak_id", "RPM").transpose()
        df.index.name = None

        # normalise to upper quantiles for between sample comparison
        df = df.div(df.quantile(q=0.75, axis=0), axis=1)

        return df
    
    counts = get_counts(statement)
    counts.columns = sample_info["category"]
    
    # use sample information to get no. replicates & conditions
    rep_pairs = sample_info.pivot("condition", "replicate", "category").transpose()
    rep_pairs.columns.name = None
    rep_pairs.index.name = None

    # report replicates to dict
    reps = {}
    for col in rep_pairs.columns:
        reps[col]=[rep_pairs[col].iloc[0], rep_pairs[col].iloc[1]]

    sns.set(style="whitegrid", palette="muted")# set seaborn theme

    # use dict to subset df of normalised counts & plot rep correlations
    for key in reps:
        df = counts[reps[key]]
        df.columns = ["Rep1", "Rep2"]
        p = sns.jointplot(data=df, y="Rep1", x="Rep2", kind="reg", size=7, color="g")
        plt.subplots_adjust(top=0.9)
        p.fig.suptitle(key) # add title
        out = "QC_plots/pearsonRepCorr_" + str(key) + ".png"
        p.savefig(out)# save fig to file here


@follows(loadgetSampleInfo, mappingPlots, insertSizePlots, peakCallingPlots, mergedPeaksetPlots)
def QCplots():
    pass


@follows(loadgetSampleInfo)
@files(None, "regulated_genes.dir/TSS.bed")
def TSSbed(infile, outfile):
    '''Get TSSs for all genes'''
    
    query = '''select distinct contig, start, end, gene_name, strand
                  from ensemblGeneset''' 

    # db = PARAMS["database"]
    
    # TSS = DB.fetch_DataFrame(query, db)

    # TSS.to_csv(outfile, sep="\t", header=False, index=False)

    
    dbh = sqlite3.connect(PARAMS["database"])
    cc = dbh.cursor()
    sqlresult = cc.execute(query).fetchall()

    o = open(outfile,"w")
    
    for r in sqlresult:
        contig, start, end, gene_id, gene_strand = r[0:5]
       
        if gene_strand == "+": gstrand = 1
        else: gstrand = 2

        tss = getTSS(start,end,gene_strand)

        tss_start = tss -1
        tss_end = tss +1
        
        columns = [ str(x) for x in
                    [  contig, tss_start, tss_end, gene_id] ]
        o.write("\t".join( columns  ) + "\n")
    o.close()

    
@transform(TSSbed,
           regex("regulated_genes.dir/TSS.bed"),
           add_inputs("deeptools.dir/*.coverage.bw"),
           r"deeptools.dir/TSS.matrix.gz")
def TSSmatrix(infiles, outfile):

    #bed, bws = infiles
    bed = infiles[0]
    bws = [x for x in infiles if ".bw" in x]
    
    job_threads = str(len(bws))

    bws = ' '.join(bws)
    
    statement = '''computeMatrix reference-point
                     -S %(bws)s
                     -R %(bed)s
                     --missingDataAsZero
                     -bs 10
                     -a 2500
                     -b 2500
                     -p "max"
                     -out %(outfile)s'''

    P.run()

    
@transform(TSSmatrix,
           suffix(".matrix.gz"),
           r"\1_profile.png")
def TSSprofile(infile, outfile):
    '''Plot profile over TSS'''

    statement = '''plotProfile
                     -m %(infile)s
                     --perGroup
                     --plotTitle "TSS enrichment"
                     --yAxisLabel "ATAC signal (RPKM)"
                     -out %(outfile)s'''

    P.run()

    
@follows(TSSprofile)
def TSSplot():
    pass        
    
    
# ---------------------------------------------------
# Generic pipeline tasks
@follows(mapping, peakcalling, coverage, frip, count, QCplots, TSSplot)
def full():
    pass


@follows(mkdir("report"))
def build_report():
    '''build report from scratch.

    Any existing report will be overwritten.
    '''

    E.info("starting report build process from scratch")
    P.run_report(clean=True)


@follows(mkdir("report"))
def update_report():
    '''update report.

    This will update a report with any changes inside the report
    document or code. Note that updates to the data will not cause
    relevant sections to be updated. Use the cgatreport-clean utility
    first.
    '''

    E.info("updating report")
    P.run_report(clean=False)


@follows(update_report)
def publish_report():
    '''publish report in the CGAT downloads directory.'''

    E.info("publishing report")
    P.publish_report()

if __name__ == "__main__":
    sys.exit(P.main(sys.argv))
