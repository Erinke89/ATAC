"""===========================
Pipeline MEME-ChIP
===========================

Overview
========

This pipeline runs MEME-ChIP and Homer for de novo motif discovery at provided bed files. Motif
discovery settings can be configured in pipeline.ini.

After motif discovery MAST can be run to rank input peaks by specified motifs (either de novo 
motifs discovered by meme, or database entries for known motifs, or both).

Output files are html report of motif discovery runs, a summary notebook, and bed files for 
motifs of interes in input peaks. 

Usage
=====

See :ref:`PipelineSettingUp` and :ref:`PipelineRunning` on general
information how to use CGAT pipelines.

- Use the target "runMotifAnalysis" for motif discovery and report generation
- Motifs of interest can then be speciifed in pipeline.ini and make "runMastAnalysis" target
  to run MAST on input peaks (or "full" target to run both tasks)
- Aternatively, if motif analysis was run elsewhere, MAST can be run on its own 
  to search for defined motif in new peaks etc.

Configuration
-------------

The pipeline requires a configured :file:`pipeline.ini` file.
CGATReport report requires a :file:`conf.py` and optionally a
:file:`cgatreport.ini` file (see :ref:`PipelineReporting`).

Default configuration files can be generated by executing:

   python <srcdir>/pipeline_macs2.py config

Input files
-----------
- bed formatted files

Requirements
------------
cgat-flow,
cgat-core,
cgat-apps,

MEME venv:
- use conda to create a python 2 venv with which to run meme BEFORE running pipeline
- e.g. conda create -n meme python=2.7.14 meme=4.11.2
- specify the name of meme venv in pipeline.yml


Pipeline output
===============

.. Describe output files of the pipeline here

Glossary
========

.. glossary::


Code
====

"""

from ruffus import *

import cgatcore.experiment as E
from cgatcore import pipeline as P
import cgat.FastaIterator

import PipelineMemechip as S

import sys, tempfile 
import glob, os 
import subprocess
import sys
#import rpy2.robjects as R
import logging as L
import sqlite3
import pybedtools
import pandas as pd
import matplotlib
from matplotlib import pyplot as plt
import seaborn as sns

# -----------------------------------------------
# Pipeline configuration
P.get_parameters(
		 ["%s/pipeline.yml" % os.path.splitext(__file__)[0],
		  "../pipeline.yml",
		  "pipeline.yml"],
		 )

PARAMS = P.PARAMS

db = PARAMS['database']['url'].split('./')[1]

def connect():
    '''connect to database.
    This method also attaches to helper databases.
    '''

    dbh = sqlite3.connect(db)

    if not os.path.exists(PARAMS["annotations_database"]):
        raise ValueError(
                     "can't find database '%s'" %
                     PARAMS["annotations_database"])

    statement = '''ATTACH DATABASE '%s' as annotations''' % \
    (PARAMS["annotations_database"])

    cc = dbh.cursor()
    cc.execute(statement)
    cc.close()

    return dbh


################################################
#####              Meme-ChIP               #####
################################################
@follows(connect)
@transform("data.dir/*.peaks.bed",
           regex(r"(.*).peaks.bed"),
           r"\1.meme.bed")
def peakSummit(infile, outfile):
    '''Make bed of peak_summit (+/- 1bp), sorted by peak qvalue'''

    #infile = ''.join([x for x in [infile] if "meme" not in x]) # prevent re-running of this job
    # should no longer be neccessary

    if os.path.isfile(infile):
        df = pd.read_csv(infile, sep="\t", header=None)

        if len(list(df))==5: # add summit column if missing
            df.columns = ["contig", "start", "end", "peak_id", "score"]
            df["summit"] = df["summit"] = df.apply(lambda x: int(x.start+((x.end-x.start)/2)), axis=1)
        else:
            df = df.iloc[:, 0:6] # subset on useful columns
            df.columns = ["contig", "start", "end", "peak_id", "score", "summit"]

        df = df.sort_values("score", ascending=False) # sort by score

        df["summit_start"] = df["summit"] - 1
        df["summit_end"] = df["summit"] + 1

        df = df[["contig", "summit_start", "summit_end", "peak_id", "score"]]

        df.to_csv(outfile, sep="\t", header=None, index=None)

        
def getMemeForegroundBedGenerator():
    
    beds = glob.glob("data.dir/*.meme.bed")

    print(PARAMS["memechip_npeaks"])
    
    if len(beds) == 0:
        pass

    npeaks = [str(x) for x in PARAMS["memechip_npeaks"]]
    widths = [str(x) for x in PARAMS["memechip_widths"]]

    outdir = "meme.seq.dir/"
    
    for bed in beds:
        int_name = os.path.basename(bed)[:-len(".meme.bed")]
        for n in npeaks:
            for w in widths:
                fgname = ".".join([int_name,n,w,"foreground","bed"])
                outfile = outdir + fgname

                yield [ bed, outfile ]

                
@follows(peakSummit, mkdir("meme.seq.dir/"))
@files(getMemeForegroundBedGenerator)
def getMemeForegroundBed(infile, outfile):
    '''Make meme beds of peak center +/- n bp, for n top scoring peaks'''

    chrom_sizes = PARAMS["annotations_chrom_sizes"]
    
    npeaks = os.path.basename(outfile)[:-len(".foreground.bed")].split(".")[-2]
    width = os.path.basename(outfile)[:-len(".foreground.bed")].split(".")[-1]
    start_width = (int(width) / 2) + 1 # correct for previous offset
    end_width = (int(width) / 2) - 1 # as bed file is chr, peak_centre_minus1, peak_centre_plus1
    
    if npeaks != "all":
        statement = '''slopBed 
                         -i <(head -n %(npeaks)s %(infile)s )
                         -g %(chrom_sizes)s 
                         -l %(start_width)s 
                         -r %(end_width)s 
                         -s 
                         > %(outfile)s''' 
        
    else:
        statement = '''slopBed 
                         -i  %(infile)s 
                         -g %(chrom_sizes)s 
                         -l %(start_width)s 
                         -r %(end_width)s 
                         -s > %(outfile)s'''
   
    P.run(statement)

    
@transform(getMemeForegroundBed,
           regex(r"(.*).foreground.bed"),
           r"\1.background.bed")
def getMemeBackgroundBed(infile, outfile):
    '''get bed file of peak flanking regions (of equal width to peak) for meme background model'''

    genome_idx = os.path.join(PARAMS["annotations_mm10dir"],"assembly.dir/contigs.tsv")
    
    statement ='''sort 
                    -k1,1 
                    -k2,2n 
                    %(infile)s | 
                  bedtools flank 
                    -pct 
                    -l 1 
                    -r 1 
                    -g %(genome_idx)s 
                    > %(outfile)s'''
    
               # -pct -l 1 -r 1 will create flanking regions = peak width

    P.run(statement)
    

@follows(getMemeBackgroundBed)
@transform("meme.seq.dir/*.bed",
           regex(r"(.*).bed"),
           r"\1.fasta")
def getMemeSequences(infile, outfile):
    '''Get the peak sequences. The genome sequences are already repeat soft-masked'''
  
    genome_fasta = os.path.join(PARAMS["genome_dir"],PARAMS["genome"]+".fasta")
    genome_idx = os.path.join(PARAMS["annotations_mm10dir"],"assembly.dir/contigs.tsv")

    statement = '''fastaFromBed 
                     -fi %(genome_fasta)s 
                     -bed %(infile)s 
                     -fo %(outfile)s'''
    
    P.run(statement)
    

@follows(getMemeSequences)
@transform("meme.seq.dir/*background.fasta", suffix(".fasta"), ".bfile")
def getMemeBfiles(infile, outfile):
    '''prepare the meme background model'''

    statement='''fasta-get-markov 
                   -m 2 %(infile)s  
                   > %(outfile)s''' 

    P.run(statement)
    
               
@follows(getMemeSequences, getMemeBfiles, mkdir("meme.chip.dir"))
#@transform(getMemeSequences,
@transform("meme.seq.dir/*.foreground.fasta",
           regex(r"meme.seq.dir/(.*).foreground.fasta"),
           r"meme.chip.dir/\1.memechip")
def runMemeChIP(infile, outfile):
    '''run MemeChIP'''

    outdir = outfile.replace(".memechip", "")
    bfile= infile.replace(".foreground.fasta", ".background.bfile")

    motifDb =  " -db ".join(PARAMS["memechip_motif_db"]) # Meme-Chip needs each db in list to have "-db" flag

    nmotifs = PARAMS["memechip_nmotif"]
    meme_max_jobs = PARAMS["memechip_meme_maxsize"]

    # Meme options:
    
    # nmeme - The upper bound on the number of sequences that are passed to MEME.
            # This is required because MEME takes too long to run for very large sequence sets.
            # All input sequences are passed to MEME if there are not more than limit.
            # default nmeme = 600
            # specified in pipeline.ini
        
    # ccut - The maximum length of a sequence to use before it is trimmed to a central region of this size.
            # A value of 0 indicates that sequences should not be trimmed.

    # meme-maxsize - Change the largest allowed dataset to be size.
            # default meme-maxsize is 100,000.
            # Fine with the default settings for -nmeme (600) and -ccut (100), largest possible dataset size would be 60000.
            # 600,000, equivalent to max of 600 1000bp seq

    # mode - specified in pipeline.ini
    # oops - One Occurence Per Seqeunce - meme assumes that each sequence only contains 1 occurence of each motif
    #      - fastest and most sensitive, but motifs returned may be "blurry" if any sequences lack them
    # zoops - Zero or One Occurence Per Sequence - meme assumes that each sequence contains at most one occurence of each motif
    #       - takes twice as long as oops, useful when motif may not be present in all sequences, but less sensitive to weak motifs present in all sequences
    # anr - Any Numer of Repetitions - meme assumes that each sequence contains any number of non-overlapping occurences of each motif
    #     - Useful if motifs repeat multiple times within a sequence. If this is the case then will be much more sensitve than oops & zoops.
    #     - Takes 10x more computing power than option 1 and is less sensitive to weak, non-repeated motifs
        
    mode = PARAMS["memechip_mode"]

    env = PARAMS["memechip_env"]
    
    statement = '''nmeme=`wc -l %(infile)s | tr -s "[[:blank:]]" "\\t" | cut -f1` &&
                   meme-chip
                     -oc %(outdir)s
                     -db %(motifDb)s
                     -bfile %(bfile)s
                     -ccut 0
                     -nmeme $nmeme
                     -meme-mod %(mode)s
                     -meme-minw 5
                     -meme-maxw 30
                     -meme-nmotifs %(nmotifs)s
                     -meme-maxsize %(meme_max_jobs)s
                     %(infile)s
                     > %(outfile)s ''' 

    # python3 version of tomtom does exist but isn't called by meme-chip
    # therefore run in python 2 conda env

    P.run(statement, job_condaenv=env, job_memory="2G", job_threads=5)

    
def loadMemeTomTomGenerator():

    meme_tomtom = glob.glob("meme.chip.dir/*memechip")

    if len(meme_tomtom) == 0:
        pass
        
    for tomtom in meme_tomtom:
        basename = os.path.basename(tomtom)[:-len(".memechip")]
        infile = tomtom.replace(".memechip", "/meme_tomtom_out/tomtom.txt")
        outfile = infile[:-len("tomtom.txt")] + basename + "_Meme_tomtom.load"

        if os.path.isfile(infile): 
            # some empty files are generated w/ just headers, causing errors loading empty tables
            # count lines in file, if file more than 1 line, yield job
            with open(infile, "r") as o:
                n = 0
                for line in o:
                    n = n + 1
                if n > 1:
                    yield [ infile, outfile ]

                    
@follows(runMemeChIP)
@files(loadMemeTomTomGenerator)
def loadMemeTomTom(infile, outfile):
    '''load meme tomtom results'''

    # cgat default is not to submit P.load() jobs to cluster
    # this is hardcoded in P.load()

    tablename = os.path.basename(outfile).replace(".load", "").replace(".", "_")
    options='-H "query_id,target_id,optimal_offset,p_value,e_value,q_value,overlap,query_consensus,targe_consensus,orientation" '

    statement = []
    statement.append('''cat %(infile)s | ''')
    statement.append(P.build_load_statement(tablename, options=options, retry=True) )
    statement.append(''' > %(outfile)s''')
    statement = ' '.join(statement)
    
    to_cluster = True

    P.run(statement)
    
    
def loadDremeTomTomGenerator():

    meme_tomtom = glob.glob("meme.chip.dir/*memechip")
    
    if len(meme_tomtom) == 0:
        pass
            
    for tomtom in meme_tomtom:
        basename = os.path.basename(tomtom)[:-len(".memechip")]
        infile = tomtom.replace(".memechip", "/dreme_tomtom_out/tomtom.txt")
        outfile = infile[:-len("tomtom.txt")] + basename + "_Dreme_tomtom.load"

        if os.path.isfile(infile):
            with open(infile, "r") as o:
                n = 0
                for line in o:
                    n = n + 1
                if n > 1:
                    yield [ infile, outfile ]

                    
@follows(loadMemeTomTom)
@files(loadDremeTomTomGenerator)
def loadDremeTomTom(infile, outfile):
    '''load dreme tomtom results'''

    to_cluster = True

    tablename = os.path.basename(outfile).replace(".load", "").replace(".", "_")
    options = '-H "query_id,target_id,optimal_offset,p_value,e_value,q_value,overlap,query_consensus,targe_consensus,orientation" '

    statement = []
    statement.append('''cat %(infile)s | ''')
    statement.append(P.build_load_statement(tablename, options=options, retry=True) )
    statement.append(''' > %(outfile)s''')
    statement = ' '.join(statement)
    
    P.run(statement)

    
def summarizeFimoGenerator():

    memes = glob.glob("meme.chip.dir/*/fimo_out*")

    jobs = []
    
    if len(memes) == 0:
        pass

    n = 0
    for meme in memes:
        n = n + 1
        meme_dir = "/".join(meme.split("/")[0:2]) + "/fimo_out*/fimo.txt"
        outfile = "/".join(meme_dir.split("/")[0:2])+ "/" + meme_dir.split("/")[1] + "_fimo_summary.txt"
        fimo_dirs = glob.glob(str(meme_dir))

        if len(fimo_dirs)==1:
            fimo_dirs = ''.join(fimo_dirs)
        else:
            fimo_dirs = ' '.join(fimo_dirs)
        
        pairs = [ fimo_dirs, outfile ]
        jobs.append(pairs)
        
    unique_jobs = [list(x) for x in set(tuple(x) for x in jobs)] # remove duplicates from list

    for j in unique_jobs:
        infiles = j[0].split(" ")
        outfile = j[1]

        not_empty_infiles = []
        for infile in infiles:
            if os.path.isfile(infile):
                with open(infile, "r") as o:
                    n = 0
                    for line in o:
                        n = n + 1
                if n > 1:
                    not_empty_infiles.append(infile)

        if len(not_empty_infiles)>0:
            yield [not_empty_infiles, outfile]

            
@follows(loadDremeTomTom)    
@files(summarizeFimoGenerator)
def summarizeFimo(infiles, outfile):
    '''summarize fimo results, remove empty files'''

    infiles = ' '.join(infiles)

    tmp_dir = "$SCRATCH_DIR"

    statement = '''tmp=`mktemp -p %(tmp_dir)s` &&
                   cat %(infiles)s | 
                     grep -v "#" | 
                     sort -k1,1n > $tmp &&
                   [[ -s $tmp ]] && mv $tmp %(outfile)s || rm $tmp'''

    P.run(statement)

    
@transform(summarizeFimo, suffix(".txt"), ".load")
def loadFimo(infile, outfile):
    
    to_cluster = True

    tablename = os.path.basename(outfile).replace(".load", "").replace(".", "_")
    options='-H "pattern_name,sequence_name,start,stop,strand,score,p_value,q_value,matched_sequence" '

    statement = []
    statement.append('''cat %(infile)s | ''')
    statement.append(P.build_load_statement(tablename, options=options, retry=True) )
    statement.append(''' > %(outfile)s''')
    statement = ' '.join(statement)
    
    P.run(statement)

    
@follows(runMemeChIP, loadMemeTomTom, loadDremeTomTom, loadFimo)
def runMemeAnalysis():
    pass


################################################
#####        Find motifs with HOMER        #####
################################################
#@follows(runMemeAnalysis, mkdir("homer.chip.dir"))
@follows(mkdir("homer.chip.dir"))
@transform("meme.seq.dir/*.foreground.fasta",
           regex(r"meme.seq.dir/(.*).foreground.fasta"),
           r"homer.chip.dir/\1.homer.log")
def runHomerFindMotifs(infile, outfile):
    '''run Homer findMotifs.pl on fasta sequences'''

    outdir = outfile.replace(".homer.log", "")
    bfile = infile.replace(".foreground.fasta", ".background.fasta")

    statement = '''if [ ! -d %(outdir)s ]; 
                     then mkdir %(outdir)s; 
                     fi;
                   findMotifs.pl 
                     %(infile)s 
                     fasta 
                     %(outdir)s 
                     -fastaBg %(bfile)s
                     &> %(outfile)s''' 

    P.run(statement)

    
@follows(runHomerFindMotifs, mkdir("homer.genome.dir"))
@transform("meme.seq.dir/*foreground.bed",
           regex(r"meme.seq.dir/(.*).foreground.bed"),
           r"homer.genome.dir/\1.homer.log")
def runHomerFindMotifsGenome(infile, outfile):
    '''Run homer findMotifsGenome.pl with default settings and background'''

    # Homer requires input peaks to have strand (+/- : 0/1) in 6th column
    # Also, sequence length must be specified, default = 200

    tmp_dir = "$SCRATCH_DIR"
    outdir = outfile.replace(".homer.log", "")
   
    statement = '''peaks=`mktemp -p %(tmp_dir)s` &&
                  awk 'BEGIN {OFS="\\t"} {print $0,0}' %(infile)s > $peaks && 
                  findMotifsGenome.pl 
                    $peaks
                    mm10
                    %(outdir)s
                    -h
                    -size given
                    &> %(outfile)s &&
                  rm $peaks'''
    
    P.run(statement)
    

@follows(runHomerFindMotifsGenome, mkdir("motifsCoverage.dir"))
@transform("meme.seq.dir/*.foreground.bed",
           regex(r"meme.seq.dir/(.*).foreground.bed"),
           r"motifsCoverage.dir/\1.motifCoverage.txt")
def annotatePeaks(infile, outfile):
    '''Annotate peaks w/ top discovered motifs for histogram plots'''

    # get original input peak file for motif search
    run = os.path.basename(infile).replace(".foreground.bed", "")
    name = run.split(".")[0]
    peak_file = "data.dir/" + name + ".bed"

    # # select top motifs to search for
    # motifs = "homer.chip.dir/" + run + "/homerResults/motif[1-6].motif"

    # search for all discovered motifs
    motifs = "homer.genome.dir/" + run + "/homerResults/motif*.motif"
    
    statement = '''annotatePeaks.pl 
                     %(peak_file)s
                     mm10 
                     -size 1000 
                     -hist 5 
                     -m %(motifs)s
                     > %(outfile)s'''
    P.run(statement)
    
                  
@transform("motifsCoverage.dir/*.motifCoverage.txt",
            regex(r"(.*).motifCoverage.txt"),
            r"\1.motifEnrichment.png")
def homerMotifEnrichmentPlot(infile, outfile):
    '''Process results from annotatePeaks and plot motif enrichment relative to peaks'''

    statement = '''python motifPlot.py 
                     %(infile)s 
                     %(outfile)s'''

    P.run(statement)
    
        
@follows(runHomerFindMotifs, runHomerFindMotifsGenome, annotatePeaks)
def runHomerAnalysis():
    pass


#@follows(runMemeAnalysis)
@files(None, "*.nbconvert.html")
def report(infile, outfile):
    '''Generate html report on pipeline results from ipynb template(s)'''

    templates = PARAMS["report_path"]

    if len(templates)==0:
        print("Specify Jupyter ipynb template path in pipeline.ini for html report generation")
        pass

    for template in templates:
        infile = os.path.basename(template)
        outfile = infile.replace(".ipynb", ".nbconvert.html")
        nbconvert = infile.replace(".ipynb", ".nbconvert.ipynb")
        tmp = os.path.basename(template)
    
        statement = '''cp %(template)s .  &&
                   jupyter nbconvert 
                     --to notebook 
                     --allow-errors 
                     --ExecutePreprocessor.timeout=None
                     --ExecutePreprocessor.kernel_name=python3
                     --execute %(infile)s &&
                   jupyter nbconvert 
                     --to html 
                     --ExecutePreprocessor.timeout=None
                     --ExecutePreprocessor.kernel_name=python3
                     --execute %(nbconvert)s &&
                   rm %(tmp)s'''

        P.run(statement)

        
@follows(runMemeAnalysis, runHomerAnalysis)
def runMotifAnalysis():
    pass

###############################################################################
################ Known motif enrichment analysis (AME) #########################
###############################################################################
@follows(mkdir("meme.ame.dir"), runMemeAnalysis)
@transform(getMemeSequences,
           regex(r"meme.seq.dir/(.*).foreground.fasta"),
           r"meme.ame.dir/\1.memeame")
def runAme(infile,  outfile):
    '''Note importance of sorted input sequences...'''

    # FASTA files in meme.seq.dir are sorted by peak score
    
    nfgseq = FastaIterator.count(infile)      
    bfile = infile.replace("foreground.fasta","background.bfile") 
    outdir = outfile.replace(".memeame","")
    
    motifDb =  PARAMS["ame_motif_db"]
    
    statement='''ame
                 --verbose 1
                 --oc %(outdir)s
                 --fix-partition %(nfgseq)s
                 --scoring totalhits
                 --bgfile %(bfile)s
                 --bgformat 2
                 --method fisher
                 --length-correct
                 <( cat %(infile)s )
                 <( cat %(motifDb)s )
                 > %(outfile)s '''

    P.run(statement, job_memory="10G", job_threads=4)


###############################################################################
############### Get Peaks associated with discovered motifs ###################
###############################################################################
@follows(mkdir("meme.seq.dir"))
@follows(mkdir("query_motifs.dir/mast.results.dir"))
@active_if(bool(PARAMS["mast_meme_motif"]))
@files(None, "query_motifs.dir/denovo_motif.meme")
def getMemeMotif(infile, outfile):
    '''Get meme formatted motif from MEME-CHIP output.
       Specify motif to get in pipeline.ini (path and motif number)'''

    meme_out, motif_no, motif_name = PARAMS["mast_meme_motif"].split(",")
    max_motif = PARAMS["memechip_nmotif"]

    motif_start = "^MOTIF  " + str(motif_no)

    if int(motif_no) < max_motif:
        motif_end = "^MOTIF  " + str(int(motif_no)+1)
    else:
        motif_end = "^SUMMARY" 
        

    statement = '''sed -n '/^MEME/,/^MOTIF  1/p' %(meme_out)s | 
                     head -n-1 > %(outfile)s ; 
                   sed -n '/%(motif_start)s/,/%(motif_end)s/p' %(meme_out)s | 
                     head -n -1 >> %(outfile)s'''

    P.run(statement)
    
    
@follows(getMemeMotif)
@files(None, "query_motifs.dir/db_motifs.txt")
def filterTFDatabases(infile, outfile):
    '''Filter TF databases for interesting motifs identified by meme-chip
       & specified in pipeline.ini'''

    TFdb = PARAMS["mast_motif_db"]
    TFdb = TFdb.replace(",", " ")
    motifs = PARAMS["mast_motifs"].split(",")

    
    for TF in motifs:
    
        statement = '''awk '/^MOTIF/ {p=0} /%(TF)s/ {p=1} p' 
                         <( cat %(TFdb)s ) 
                         >> %(outfile)s'''

        P.run(statement)

        
@transform(filterTFDatabases, suffix(".txt"), r".meme")
def addMemeMotifHeader(infile, outfile):
    '''prepend header to query motif .meme file'''

    TFdb = PARAMS["mast_motif_db"].split(",")[0]

    statement = '''head -n9 %(TFdb)s | 
                     cat - %(infile)s 
                       > tmp && 
                     mv tmp %(outfile)s'''

    P.run(statement)

    
@follows(addMemeMotifHeader)
@transform("data.dir/*.peaks.bed",
           regex(r"data.dir/(.*).peaks.bed"),
           r"meme.seq.dir/\1.input_sequences.fasta")
def getInputPeakSequences(infile, outfile):
    '''Convert input bed files (all peaks) to fasta sequences for motif searching with MAST & FIMO'''

    genome_fasta = os.path.join(PARAMS["genome_dir"],PARAMS["genome"]+".fasta")

    statement = '''fastaFromBed 
                     -name 
                     -fi %(genome_fasta)s 
                     -bed %(infile)s 
                     -fo %(outfile)s'''

    P.run(statement)

    
@active_if(PARAMS["mast_background"])
@transform("data.dir/*.peaks.bed",
           regex(r"data.dir/(.*).peaks.bed"),
           r"meme.seq.dir/\1.input_background.fasta")
def getInputPeakBackgroundFASTA(infile, outfile):
    '''get bed file of peak flanking regions (of equal width to peak) for meme background model'''

    genome_idx = os.path.join(PARAMS["annotations_mm10dir"],"assembly.dir/contigs.tsv")
    genome_fasta = os.path.join(PARAMS["genome_dir"],PARAMS["genome"]+".fasta")
    tmp_dir = "$SCRATCH_DIR"
    
    statement ='''tmp=`mktemp -p %(tmp_dir)s` &&
                  sort -k1,1 -k2,2n %(infile)s | 
                    bedtools flank 
                      -pct 
                      -l 1 
                      -r 1 
                      -g %(genome_idx)s 
                      > $tmp &&
                  fastaFromBed 
                    -name 
                    -fi %(genome_fasta)s 
                    -bed $tmp 
                    -fo %(outfile)s &&
                  rm $tmp'''
    
    P.run(statement)
    
    
@active_if(PARAMS["mast_background"])
@transform(getInputPeakBackgroundFASTA, suffix(".fasta"), ".bfile")
def getInputPeakBackgroundModel(infile, outfile):
    '''prepare the meme background model'''

    statement='''fasta-get-markov 
                   -m 2 
                   %(infile)s  
                   > %(outfile)s'''

    P.run(statement)

    
@follows(getInputPeakSequences, getInputPeakBackgroundModel)
@transform("meme.seq.dir/*.input_sequences.fasta",
           regex(r"meme.seq.dir/(.*).input_sequences.fasta"),
           add_inputs(["query_motifs.dir/*.meme"]),
           r"query_motifs.dir/mast.results.dir/\1.mast.log")
def runMast(infiles, outfile):
    '''Run mast for selected motifs (in pipeline.ini file, output hit coordinated at *.mast.txt'''
    
    sequences, motifs = infiles

    background = PARAMS["mast_background"]

    if background == "custom":
        bfile = sequences.replace(".input_sequences.fasta", ".input_background.bfile")
        b_opt = '''-bfile %(bfile)s''' % locals()
    else:
        b_opt = ''' '''
        
    for motif in motifs:

        if len(motif) > 0:
            suffix = os.path.basename(motif).replace(".meme", "") 
            outdir = '.'.join([outfile[:-len(".mast.log")], suffix])

            statement = '''mast 
                           -w              
                           %(b_opt)s
                           -oc %(outdir)s
                           <(cat %(motif)s )
                           %(sequences)s
                           &> %(outfile)s '''

            P.run(statement, job_memory="5G", job_threads=4)

            
@follows(runMast)
@transform("data.dir/*.peaks.bed",
           regex("data.dir/(.*).bed"),
           r"data.dir/\1.load")
def loadPeaks(infile, outfile):
    '''Load input peaks to merge w/ MAST results by peak_id'''

    tablename = os.path.basename(outfile).replace(".load", "").replace(".", "_")
    options='-H "contig,start,end,peak_id,score" '

    statement = []
    statement.append('''cat %(infile)s | cut -f1-5 - |''')
    statement.append(P.build_load_statement(tablename, options=options, retry=True) )
    statement.append(''' > %(outfile)s''')
    statement = ' '.join(statement)
    
    to_cluster = True

    P.run(statement)

    
@follows(loadPeaks, mkdir("query_motifs.dir/mast.beds.dir/"))
@transform("query_motifs.dir/mast.results.dir/*/mast.txt",
           regex("query_motifs.dir/mast.results.dir/(.*)/mast.txt"),
           r"query_motifs.dir/mast.results.dir/\1/mast.load")
def loadMast(infile, outfile):
    '''MAST results section1 contains high scoring sequences (for input motifs), 
       ranked by increasing e-value (up to a max of 10)'''
    
    tmp_dir = "$SCRATCH_DIR"
    tablename = "MAST_" + outfile.split("/")[-2].replace(".", "_")
    options='-H "peak_id,e_value,length" '

    statement = []
    statement.append('''tmp=`mktemp -p %(tmp_dir)s` &&
                        sed -n '/^SECTION I:/,/^SECTION II:/p' %(infile)s | 
                          grep "^[a-zA-Z].*[0-9]$" - | 
                          tr -s "[[:blank:]]" "\\t" 
                          > $tmp &&
                        cat $tmp ''')
    statement.append(P.build_load_statement(tablename, options=options, retry=True))
    statement.append(''' > %(outfile)s && rm $tmp''')    
    statement = '|'.join(statement)

    P.run(statement)
    
    
@transform(loadMast,
           regex("query_motifs.dir/mast.results.dir/(.*)/mast.load"),
           r"query_motifs.dir/mast.beds.dir/\1.topMASTpeaks.bed")
def mast_table(infile, outfile):
    '''get mast results as bed file.
       e.g. top scoring peaks for query motifs'''

    db = PARAMS["database"]
    mast_table = "MAST_" + infile.split("/")[2].replace(".", "_")
    peak_table = infile.split("/")[2].split(".")[0] + "_peaks"

    query = '''select a.contig, a.start, a.end, a.peak_id, b.e_value 
               from %(peak_table)s a, %(mast_table)s b where a.peak_id = b.peak_id''' % locals()

    df = Database.fetch_DataFrame(query, db)
    df.to_csv(outfile, header=False, index=None, sep="\t")
    

@follows(mast_table)
@transform("meme.seq.dir/*.input_sequences.fasta",
           regex(r"meme.seq.dir/(.*).input_sequences.fasta"),
           add_inputs(["query_motifs.dir/*.meme"]),
           r"query_motifs.dir/mast.results.dir/\1.mast.hit_list.log")
def runMast_HitList(infiles, outfile):
    '''Run mast for selected motifs (in pipeline.ini file), 
       output hit coordinated at *.mast.txt'''
    
    sequences, motifs = infiles

    background = PARAMS["mast_background"]
    if background == "custom":
        bfile = sequences.replace(".input_sequences.fasta", ".input_background.bfile")
        b_opt = '''-bfile %(bfile)s''' % locals()
    else:
        b_opt = ''' '''
         
    for motif in motifs:
        
        suffix = os.path.basename(motif).replace(".meme", "") 
        outdir = '.'.join([outfile[:-len(".mast.hit_list.log")], suffix])
        hlist = outdir + "/mast.hit_list.txt"

        statement = '''mast 
                         -hit_list
                         -best
                         %(b_opt)s
                         -w              
                         -oc %(outdir)s
                         <(cat %(motif)s )
                         %(sequences)s
                         > %(hlist)s
                         2>> %(outfile)s '''
        
        P.run(statement, job_memory="5G", job_threads=4)
        

@follows(runMast_HitList)
@transform(filterTFDatabases, suffix(".txt"), r".table.txt")
def motifTable(infile, outfile):
    '''count motifs in .meme db'''

    statement = '''cat %(infile)s | 
                     grep MOTIF | 
                     tr -s " " | 
                     sed 's/ /\\t/g' | 
                     awk 'BEGIN {OFS="\\t"} {print $2,$3,NR}' 
                     > %(outfile)s'''

    P.run(statement)
    

@transform(motifTable, suffix(".txt"), r".load")
def loadmotifTable(infile, outfile):
    P.load(infile, outfile, options='-H "motif_id,motif_name,motif_no" ')

    
@follows(loadmotifTable)
@transform(runMast_HitList,
           regex(r"query_motifs.dir/mast.results.dir/(.*).mast.hit_list.log"),
           r"query_motifs.dir/mast.results.dir/\1.mast.hit_list.touch")
def HitList_table(infile, outfile):
    '''make table for csvdb'''

    head, tail = os.path.split(infile)
    infile1 = head + "/" + tail.split(".")[0] + ".denovo_motif/" + "mast.hit_list.txt"
    infile2 = head + "/" + tail.split(".")[0] +".db_motifs/" + "mast.hit_list.txt"
    infiles = [infile1, infile2]

    for infile in infiles:

        out = infile.replace(".txt", ".table.txt")

        statement = []
        statement.append('''grep -v "#" %(infile)s | 
                              tr -s "[[:blank:]]" "\\t" | 
                              sort -k5,5rn | 
                              sed 's/\([+-]\)\([0-9]\)/\\1\\t\\2/' ''')
        
        if "denovo" in infile:
            ### check what needs to be specified here - it should not be necessary
            ### as motif files can be added into data.dir/ and should be named accordingly
            motif_name = PARAMS["mast_meme_motif"].split(",")
            cmd_suffix = ''' sed 's/\(\\t1\\t\)/\\t%(motif_name)s\\t/' ''' % locals()
            statement.append(cmd_suffix)
            
        if os.path.exists(infile):
            statement = '|'.join(statement) + ''' > %(out)s'''
            P.run(statement)
            
        else:
            message = '''%(infile)s does not exist''' % locals()
            print(message)

    P.touch(outfile)

    
@follows(HitList_table)
@transform("query_motifs.dir/mast.results.dir/*/mast.hit_list.table.txt",
           regex(r"query_motifs.dir/mast.results.dir/(.*)/mast.hit_list.table.txt"),
           r"query_motifs.dir/mast.results.dir/\1/mast.hit_list.table.load")
def loadHitList_table(infile, outfile):
    table = "MASThitlist_" + infile.split("/")[2]
    P.load(infile, outfile, tablename=table , options=' -H "peak_id,strand,motif_no,hit_start,hit_end,score,hit_p_value" ')

    
@follows(loadHitList_table)
@transform(runMast_HitList,
           regex(r"(.*).mast.hit_list.log"),
           add_inputs(["query_motifs.dir/*.meme"]),
           r"\1.mast.hit_list.results.touch")
def mastHitList_results(infiles, outfile):
    '''Make informative table with mast hit list results'''

    infile, motifs = infiles

    dfs = []
    n = 0
    for motif in motifs:
        if len(motif)>0:
            n = n + 1
            
            mname = os.path.basename(motif).replace(".meme", "")
            bedfile = os.path.basename(infile).replace(".mast.hit_list.log", "").replace(".", "_")

            hit_list_table = "MASThitlist_" + '_'.join([bedfile, mname])
            peak_table = bedfile + "_peaks"
            motif_id_table = "db_motifs_table"
            
            if mname == "denovo_motif":
                query = '''select b.contig, b.start + a.hit_start as start, b.start + a.hit_end as end, 
                           a.peak_id, a.score as mast_score, a.strand, a.motif_no as motif_name, a.hit_p_value as p_value 
                           from %(hit_list_table)s a, %(peak_table)s b 
                           where a.peak_id = b.peak_id order by mast_score desc''' % locals()

            elif mname == "db_motifs":
                query = '''select b.contig, b.start + a.hit_start as start, b.start + a.hit_end as end, 
                           a.peak_id, a.score as mast_score, a.strand, c.motif_name, a.hit_p_value as p_value 
                           from %(hit_list_table)s a inner join %(peak_table)s b 
                           on a.peak_id = b.peak_id inner join %(motif_id_table)s c 
                           on a.motif_no = c.motif_no order by mast_score desc''' % locals()

            else:
                error_message = "%(motif)s file not recognised" % locals()
                print(error_message)

            if n == 1:
                res = Database.fetch_DataFrame(query, db)
                res.columns = [ "contig", "motif_start", "motif_end", "peak_id", "score", "strand", "motif_name", "p_value" ]
            else:
                df = Database.fetch_DataFrame(query, db)
                df.columns = [ "contig", "motif_start", "motif_end", "peak_id", "score", "strand", "motif_name", "p_value" ]
                res = res.append(df)

    mast_motifs = res["motif_name"].drop_duplicates()

    for m in mast_motifs:
        out_dir = "query_motifs.dir/mast.beds.dir/"
        filename = out_dir + '.'.join([bedfile, m]) + ".MASThitList.bed"
        df = res[res["motif_name"] == m]
        df.to_csv(filename, header=False, index=None, sep="\t")

    P.touch(outfile)
    

@follows(mastHitList_results)
def runMastAnalysis():
    pass


###############################################################################
########################### Pipeline Targets ##################################
###############################################################################
@follows(runMotifAnalysis, runMastAnalysis, report)
def full():
    pass

if __name__== "__main__":
    sys.exit( P.main(sys.argv) )
