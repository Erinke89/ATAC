##############################################################################
#
#   MRC FGU CGAT
#
#   $Id$
#
#   Copyright (C) 2009 Andreas Heger
#
#   This program is free software; you can redistribute it and/or
#   modify it under the terms of the GNU General Public License
#   as published by the Free Software Foundation; either version 2
#   of the License, or (at your option) any later version.
#
#   This program is distributed in the hope that it will be useful,
#   but WITHOUT ANY WARRANTY; without even the implied warranty of
#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#   GNU General Public License for more details.
#
#   You should have received a copy of the GNU General Public License
#   along with this program; if not, write to the Free Software
#   Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
###############################################################################
"""===========================
Pipeline template
===========================

:Author: Andreas Heger
:Release: $Id$
:Date: |today|
:Tags: Python

.. Replace the documentation below with your own description of the
   pipeline's purpose

Overview
========

This pipeline computes the word frequencies in the configuration
files :file:``pipeline.ini` and :file:`conf.py`.

Usage
=====

See :ref:`PipelineSettingUp` and :ref:`PipelineRunning` on general
information how to use CGAT pipelines.

Configuration
-------------

The pipeline requires a configured :file:`pipeline.ini` file.
CGATReport report requires a :file:`conf.py` and optionally a
:file:`cgatreport.ini` file (see :ref:`PipelineReporting`).

Default configuration files can be generated by executing:

   python <srcdir>/pipeline_macs2.py config

Input files
-----------
- Bam files & bai indexes

- sample files should be of format: <exp>-<cond>-<treatment>-<sample>.genome.bam
- with matching file for input:     <exp>-<cond>-<treatment>-<WCE>.genome.bam
- if only 1 input is needed for all samples it can be specified in pipeline.ini

Requirements
------------

The pipeline requires the results from
:doc:`pipeline_annotations`. Set the configuration variable
:py:data:`annotations_database` and :py:data:`annotations_dir`.

On top of the default CGAT setup, the pipeline requires the following
software to be in the path:

.. Add any additional external requirements such as 3rd party software
   or R modules below:

Requirements:

* samtools >= 1.1

Pipeline output
===============

.. Describe output files of the pipeline here

Glossary
========

.. glossary::


Code
====

"""
from ruffus import *

import sys
import os
import sqlite3
import CGAT.Experiment as E
import CGATPipelines.Pipeline as P
import CGAT.BamTools as BamTools
import CGAT.Database as DB
import re
import glob

# load options from the config file
PARAMS = P.getParameters(
    ["%s/pipeline.ini" % os.path.splitext(__file__)[0],
     "../pipeline.ini",
     "pipeline.ini"])

# add configuration values from associated pipelines
#
# 1. pipeline_annotations: any parameters will be added with the
#    prefix "annotations_". The interface will be updated with
#    "annotations_dir" to point to the absolute path names.
PARAMS.update(P.peekParameters(
    PARAMS["annotations_dir"],
    "pipeline_annotations.py",
    on_error_raise=__name__ == "__main__",
    prefix="annotations_",
    update_interface=True))


# if necessary, update the PARAMS dictionary in any modules file.
# e.g.:
#
# import CGATPipelines.PipelineGeneset as PipelineGeneset
# PipelineGeneset.PARAMS = PARAMS
#
# Note that this is a hack and deprecated, better pass all
# parameters that are needed by a function explicitely.

# -----------------------------------------------
# Utility functions
def connect():
    '''utility function to connect to database.

    Use this method to connect to the pipeline database.
    Additional databases can be attached here as well.

    Returns an sqlite3 database handle.
    '''

    dbh = sqlite3.connect(PARAMS["database"])
    statement = '''ATTACH DATABASE '%s' as annotations''' % (
        PARAMS["annotations_database"])
    cc = dbh.cursor()
    cc.execute(statement)
    cc.close()

    return dbh

def isPaired(files):
    '''Check whether input files are single or paired end
       Note: this is dependent on files having correct suffix'''
    
    paired = []

    for fastq in files:
        Fpair = re.findall(".*.fastq.1.gz", fastq)
        paired = paired + Fpair

    if len(paired)==0:
        unpaired = True

    else:
        unpaired = False
    
    return unpaired


def writeGreat(locations,basalup,basaldown,maxext,outfile,half=False):
    ''' write out a bed file of great promoters from input gene locations
         locations is [contig,gstart,gend,strand,gene_id] '''

    # Gene regulatory domain definition: 
    # Each gene is assigned a basal regulatory domain of a 
    # minimum distance upstream and downstream of the TSS 
    # (regardless of other nearby genes). 
    # The gene regulatory domain is extended in both directions 
    # to the nearest gene's basal domain but no more than the 
    # maximum extension in one direction

    genome = {}
    for location in locations:
        chrom, gstart, gend, strand_int, gid = location
        if strand_int == -1: 
            strand = "minus" 
            tss = gend
        else: 
            strand = "plus"
            tss = gstart
        record = [tss,strand,gid]
        if chrom[3:5]=="NT" or chrom[0:2]=="MT": continue
        if chrom not in genome: 
            genome[chrom] = [ record ]
        else: genome[chrom].append(record)

    #add the ends of the chromosomes
    contigs = gzip.open(PARAMS["annotations_dir"]+"/assembly.dir/contigs.bed.gz","r")

    
    nmatched = 0
    for contig_entry in contigs:
        contig, start, end = contig_entry.strip().split("\t")
        
        if contig in genome.keys():
            genome[contig].append([int(end),"end","end"])
            nmatched+=1
    if nmatched < 21:
        raise ValueError("writeGreat: not enough chromosome ends registered")

    #sort the arrays
    for key in genome.keys():
        genome[key].sort( key = lambda entry: entry[0] )
        
    #now we can walk over the regions and make the regulatory regions.

    greatBed = []
   
    for contig in genome.keys():

        locs = genome[contig]
        contig_end = locs[-1][0]
        for i in range(0,len(locs)):

            l,strand,gid = locs[i]

            if strand == "end": continue

            #get the positions of the neighbouring basal domains.

            # - upstream
            if i == 0: frontstop = 0
            else:
                pl, pstrand, pgid = locs[i-1]
                if pstrand == "plus": frontstop = pl + basaldown
                else: frontstop = pl + basalup
            # - downstream
            nl, nstrand, ngid = locs[i+1]
            if nstrand == "plus": backstop = nl - basalup
            else: backstop = nl - basaldown

            # define basal domain
            if strand=="plus":
                basalstart = l - basalup
                basalend = min( l + basaldown, contig_end )
            else:
                basalstart = l - basaldown
                basalend = min( l + basalup, contig_end )

            # upstream extension
            if frontstop > basalstart:
                regstart = basalstart
            else:
                if half == True:
                    frontext = min( maxext, (l - frontstop) / 2 )
                else:
                    frontext = min( maxext, l - frontstop )
                regstart = l - frontext

            # downstream extension
            if backstop < basalend:
                regend = basalend
            else:
                if half == True:
                    backext = min( maxext, ( backstop - l ) / 2 )
                else:
                    backext = min( maxext, backstop - l )
                regend = l + backext

            greatBed.append(["chr"+contig,str(regstart),str(regend),gid])
        
    outfh = open(outfile,"w")
    outfh.write("\n".join(["\t".join(x) for x in greatBed])+"\n")
    outfh.close()



def getTSS(start,end,strand):
    if strand == 1 or strand == "+": tss = start
    elif strand == -1 or strand == "-": tss = end
    else: raise ValueError("getTSS: stand specification not understood")
    return tss

# ---------------------------------------------------
# Specific pipeline tasks

# Configure pipeline for paired or single end data

Unpaired = isPaired(glob.glob("data.dir/*fastq*gz"))

# if len(PARAMS["bowtie2_unpaired"])==0:
#        pass
# else:
#        Unpaired = PARAMS["bowtie2_unpaired"]
       
#####################################################
####                Mapping                      ####
#####################################################
@follows(connect, mkdir("bowtie2.dir"))
@transform("data.dir/*.fastq.1.gz",
           regex(r"data.dir/(.*).fastq.1.gz"),
           r"bowtie2.dir/\1.genome.bam")
def mapBowtie2_PE(infile, outfile):
    '''Map reads with Bowtie2'''

    if len(infile) == 0:
        pass

    read1 = infile
    read2 = infile.replace(".1.gz", ".2.gz")
    
    job_memory = "2G"
    job_threads = "12"
    
    log = outfile + "_bowtie2.log"
    tmp_dir = "$SCRATCH_DIR"

    options = PARAMS["bowtie2_options"]
    genome = os.path.join(PARAMS["bowtie2_genomedir"], PARAMS["bowtie2_genome"])

    statement = '''tmp=`mktemp -p %(tmp_dir)s`; checkpoint;
                   bowtie2 
                     --quiet 
                     --threads 12 
                     -x %(genome)s
                     -1 %(read1)s 
                     -2 %(read2)s
                     %(options)s
                     1> $tmp 
                     2> %(log)s; checkpoint;
                   samtools sort -O BAM -o %(outfile)s $tmp; checkpoint;
                   rm $tmp''' % locals()

    print statement

    P.run()

    
@transform("data.dir/*.fastq.gz",
           regex(r"data.dir/(.*).fastq.gz"),
           r"bowtie2.dir/\1.genome.bam")
def mapBowtie2_SE(infile, outfile):
    '''Map reads with Bowtie2'''

    if len(infile) == 0:
        pass

    job_memory = "2G"
    job_threads = "12"
    
    log = outfile + "_bowtie2.log"
    tmp_dir = "$SCRATCH_DIR"

    options = PARAMS["bowtie2_options"]
    genome = os.path.join(PARAMS["bowtie2_genomedir"], PARAMS["bowtie2_genome"])

    statement = '''tmp=`mktemp -p %(tmp_dir)s`; checkpoint;
                   bowtie2 
                     --quiet 
                     --threads 12 
                     -x %(genome)s
                     -U %(infile)s 
                     %(options)s
                     1> $tmp 
                     2> %(log)s; checkpoint;
                   samtools sort -O BAM -o %(outfile)s $tmp; checkpoint;
                   rm $tmp''' % locals()

    print statement

    P.run()

@follows(mapBowtie2_PE, mapBowtie2_SE)
@transform("bowtie2.dir/*.genome.bam", suffix(r".genome.bam"), r".filt.bam")
def filterBam(infile, outfile):
    '''filter bams on MAPQ >10, & remove reads mapping to chrM before peakcalling.
       Optionally filter reads based on insert size (max size specified in ini)'''

    job_memory = "10G"
    job_threads = "2"

    local_tmpdir = "$SCRATCH_DIR"

    insert_size_filter_F = PARAMS["bowtie2_insert_size"]
    insert_size_filter_R = "-" + str(insert_size_filter_F) # reverse reads have "-" prefix for TLEN

    if len(str(insert_size_filter_F))==0:
        options = ''' '''
    else:
        options = '''awk 'BEGIN {OFS="\\t"} {if ($9 ~ /^-/ && $9 > %(insert_size_filter_R)s) print $0;
                       else if ($9 ~ /^[0-9]/ && $9 < %(insert_size_filter_F)s) print $0}' | ''' % locals()
        
    statement = '''tmp=`mktemp -p %(local_tmpdir)s`; checkpoint ;
                   head=`mktemp -p %(local_tmpdir)s`; checkpoint ;
                   samtools view -h %(infile)s | grep "^@" - > $head ; checkpoint ; 
                   samtools view -q10 %(infile)s | 
                     grep -v "chrM" - | 
                     %(options)s 
                     cat $head - |
                     samtools view -h -o $tmp - ; checkpoint ;
                   samtools sort -O BAM -o %(outfile)s $tmp ; checkpoint ;
                   rm $tmp $head''' % locals()

    print statement

    P.run()


@transform(filterBam,
           regex(r"(.*).filt.bam"),
           r"\1.prep.bam")
def removeDuplicates(infile, outfile):
    '''PicardTools remove duplicates'''

    job_memory = "5G"
    job_threads = "2"
    
    metrics_file = outfile + ".picardmetrics"
    log = outfile + ".picardlog"
    tmp_dir = "$SCRATCH_DIR"

    statement = '''tmp=`mktemp -p %(tmp_dir)s`; checkpoint ; 
                   MarkDuplicates 
                     INPUT=%(infile)s 
                     ASSUME_SORTED=true 
                     REMOVE_DUPLICATES=true 
                     QUIET=true 
                     OUTPUT=$tmp 
                     METRICS_FILE=%(metrics_file)s 
                     VALIDATION_STRINGENCY=SILENT
                     2> %(log)s ; checkpoint ;
                   mv $tmp %(outfile)s; checkpoint ; 
                   samtools index %(outfile)s'''

    print statement

    P.run()

    
@follows(removeDuplicates)
@transform("bowtie2.dir/*.bam", suffix(r".bam"), r".bam.bai")
def indexBam(infile, outfile):
    '''index bams'''

    statement = '''samtools index -b %(infile)s > %(outfile)s'''

    P.run()

@follows(indexBam)
@transform("bowtie2.dir/*.genome.bam",
           regex(r"(.*).genome.bam"),
           r"\1.contigs.counts")
def contigReadCounts(infile, outfile):
    '''count reads mapped to each contig'''

    tmp_dir = "$SCRATCH_DIR"
    name = os.path.basename(infile).rstrip(".bam")
    
    statement =  '''tmp=`mktemp -p %(tmp_dir)s`; checkpoint ;
                    samtools idxstats %(infile)s > $tmp; checkpoint;
                    awk 'BEGIN {OFS="\\t"} {print $0,"%(name)s"}' $tmp > %(outfile)s; checkpoint;
                    rm $tmp'''

    print statement

    P.run()

# @transform(contigReadCounts, suffix(r".counts"), r".load")
# def loadcontigReadCounts(infile, outfile):
#     P.load(infile, outfile, options='-H "contig,length,mapped_reads,unmapped_reads,sample_id" ')

@follows(contigReadCounts)
@merge("bowtie2.dir/*.contigs.counts", "allContig.counts")
def mergeContigCounts(infiles, outfile):

    infiles = ' '.join(infiles)
    
    statement = '''cat %(infiles)s > %(outfile)s'''

    P.run()

@transform(mergeContigCounts, suffix(r".counts"), r".load")
def loadmergeContigCounts(infile, outfile):
    P.load(infile, outfile, options='-H "contig,length,mapped_reads,unmapped_reads,sample_id" ')

    
@follows(loadmergeContigCounts)
@transform("bowtie2.dir/*.bam", suffix(r".bam"), r".flagstats.txt")
def flagstatBam(infile, outfile):
    '''get samtools flagstats for bams'''

    statement = '''samtools flagstat %(infile)s > %(outfile)s'''

    print statement
    
    P.run()


@follows(flagstatBam)
@transform("bowtie2.dir/*.bam",
           regex(r"(\S+)\.(.*).bam"),
           r"\1_\2.picardAlignmentStats.txt")
def picardAlignmentSummary(infile, outfile):
    '''get aligntment summary stats with picard'''

    tmp_dir = "$SCRATCH_DIR"
    refSeq = os.path.join(PARAMS["annotations_genome_dir"], PARAMS["genome"] + ".fasta")

    job_threads = "4"
    job_memory = "5G"

#    outfile = "bowtie2.dir/" + '_'.join(os.path.basename(outfile).split(".")[0:2]) + ".picardAlignmentStats.txt"
    
    statement = '''tmp=`mktemp -p %(tmp_dir)s`; checkpoint ;
                   CollectAlignmentSummaryMetrics
                     R=%(refSeq)s
                     I=%(infile)s
                     O=$tmp; checkpoint ;
                   cat $tmp | grep -v "#" > %(outfile)s'''

    print statement

    P.run()


@merge(picardAlignmentSummary,
       "picardAlignmentSummary.load")
def loadpicardAlignmentSummary(infiles, outfile):
    '''load the complexity metrics to a single table in the db'''

    P.concatenateAndLoad(infiles, outfile,
                         regex_filename=".*/(.*).picardAlignmentStats",
                         cat="sample_id",
                         options='-i "sample_id"')


@follows(flagstatBam)
@transform("bowtie2.dir/*.bam",
           regex(r"(\S+)\.(.*).bam"),
           r"\1_\2.picardInsertSizeMetrics.txt")
def picardInsertSizes(infile, outfile):
    '''get aligntment summary stats with picard'''

    tmp_dir = "$SCRATCH_DIR"
    
    job_threads = "8"
    job_memory = "10G"

    pdf = outfile.replace("Metrics.txt", "Histogram.pdf")
    histogram = outfile.replace("Metrics.txt", "Histogram.txt")
    
    statement = '''tmp=`mktemp -p %(tmp_dir)s`; checkpoint ;
                   CollectInsertSizeMetrics
                     I=%(infile)s 
                     O=$tmp
                     H=%(pdf)s
                     M=0.5; checkpoint ;
                   cat $tmp | grep -A`wc -l $tmp | tr "[[:blank:]]" "\\n" | head -n 1` "## HISTOGRAM" $tmp | grep -v "#" > %(histogram)s; checkpoint;
                   cat $tmp | grep -A 2 "## METRICS CLASS" $tmp | grep -v "#" > %(outfile)s; checkpoint;
                   rm $tmp'''

    print statement

    P.run()

@merge(picardInsertSizes,
       "picardInsertSizeMetrics.load")
def loadpicardInsertSizeMetrics(infiles, outfile):
    '''load the insert size metrics to a single table in the db'''

    P.concatenateAndLoad(infiles, outfile,
                         regex_filename=".*/(.*).picardInsertSizeMetrics",
                         cat="sample_id",
                         options='-i "sample_id"')


@follows(picardInsertSizes)
@merge("bowtie2.dir/*.picardInsertSizeHistogram.txt",
       "picardInsertSizeHistogram.load")
def loadpicardInsertSizeHistogram(infiles, outfile):
    '''load the insert size metrics to a single table in the db'''

    P.concatenateAndLoad(infiles, outfile,
                         regex_filename=".*/(.*).picardInsertSizeHistogram",
                         cat="sample_id",
                         options='-i "sample_id"')

    
@follows(loadpicardAlignmentSummary, loadpicardInsertSizeMetrics, loadpicardInsertSizeHistogram)
def mapping():
    pass


####################################################
#####              Peakcalling                 #####
####################################################
@active_if(Unpaired)
@follows(mapping, mkdir("macs2.dir"))
@transform("bowtie2.dir/*.prep.bam",
           regex("bowtie2.dir/(.*).prep.bam"),
           r"macs2.dir/\1.macs2.fragment_size.tsv")
def macs2Predictd(infile, outfile):
    '''predict fragment sizes for SE ChIP'''

    job_threads = "4"
    
    options = PARAMS["macs2_se_options"]
    outdir = os.path.dirname(outfile)

    statement = '''macs2 predictd 
                     --format BAM 
                     --ifile %(infile)s 
                     --outdir %(outdir)s 
                     --verbose 2 %(options)s 
                     2> %(outfile)s'''

    P.run()

    
@active_if(Unpaired)
@transform(macs2Predictd, suffix(r".fragment_size.tsv"), r".fragment_size.txt")
def getFragmentSize(infile, outfile):
    '''Get fragment sizes from macs2 predictd'''

    sample = os.path.basename(infile).rstrip(".fragment_size.tsv")
    tmp_dir = "$SCRATCH_DIR"
    
    statement = '''tmp=`mktemp -p %(tmp_dir)s`; checkpoint;
                   echo %(sample)s `cat %(infile)s | 
                     grep "# tag size =" | 
                     tr -s " " "\\t" | 
                     awk 'BEGIN {OFS="\\t "} {print $12}'` > $tmp; checkpoint; 
                   cat $tmp | tr -s " " "\\t" > %(outfile)s; checkpoint;
                   rm $tmp'''

    print statement

    P.run()

    
@active_if(Unpaired)
@transform(getFragmentSize, suffix(r".txt"), r".load")
def loadgetFragmentSize(infile, outfile):
    P.load(infile, outfile, options='-H "sample, tag_size"')


@follows(loadgetFragmentSize)
@transform(removeDuplicates,
           regex("bowtie2.dir/(.*).prep.bam"),
           r"macs2.dir/\1.macs2.log")
def macs2callpeaks(infile, outfile):
    '''call peaks with macs2'''

    if BamTools.isPaired(infile):

        options = PARAMS["macs2_pe_options"]
        name = os.path.basename(outfile).split(".")[0]
        
        statement='''macs2 callpeak 
                       --outdir macs2.dir                  
                       %(options)s 
                       --treatment %(infile)s 
                       --name %(name)s 
                       >& %(outfile)s'''  

    else:
        # get macs2 predictd fragment lengths from csvdb
        "macs2.dir/\1.macs2.fragment_size.txt"
        table = os.path.basename(infile).replace(".fastq.1.gz", ".macs2.fragment_size").replace("-", "_").replace(".", "_")
        # table = os.path.basename(fragment_size).rstrip(".txt").replace("-", "_").replace(".", "_")

        query = '''select tag_size from %(table)s ''' % locals()

        dbh = sqlite3.connect(PARAMS["database"])
        cc = dbh.cursor()
        sqlresult = cc.execute(query).fetchall()

        fragment_length = sqlresult[0]
        fragment_length = fragment_length[0]

        # run macs2 callpeak
        job_threads = "5"

        options = PARAMS["macs2_se_options"]
        name = os.path.basename(outfile).split(".")[0]
        tmp_dir = "$SCRATCH_DIR"

        statement='''macs2 callpeak 
                       --outdir macs2.dir 
                       %(options)s 
                       --treatment %(infile)s 
                       --name %(name)s 
                       >& %(outfile)s'''  

    print statement
    
    P.run()

    
@follows(macs2callpeaks)
@files(None, "blacklist_chip.mm10.bed.gz")
def getChIPblacklist(infile, outfile):
    '''Get Ensembl ChIP blacklisted regions'''

    chip_blacklist = PARAMS["peak_filter_chip_blacklist"]
    statement = '''wget -O %(outfile)s %(chip_blacklist)s'''

    P.run()

    
@follows(getChIPblacklist)
@files(None, "blacklist_atac.mm10.bed.gz")
def getATACblacklist(infile, outfile):
    '''Get ATAC blacklist regions'''

    atac_blacklist = PARAMS["peak_filter_atac_blacklist"]
    statement = '''wget -q %(atac_blacklist)s | gzip - > %(outfile)s'''

    P.run()

    
@follows(getATACblacklist)
@transform("macs2.dir/*.narrowPeak",
           regex("(.*).narrowPeak"),
           add_inputs([getChIPblacklist, getATACblacklist]),
           r"\1.filt.bed")
def filterPeaks(infiles, outfile):
    '''subtract blacklist regions from peakset'''

    peak, blacklists = infiles

    blacklist = ' '.join(blacklists)
    
    statement = '''intersectBed -wa -v -a %(peak)s -b <(zcat %(blacklist)s ) > %(outfile)s'''

    print statement
    
    P.run()

    
@follows(filterPeaks)
def peakcalling():
    pass


########################################################
####              Coverage tracks                   ####
########################################################
@follows(peakcalling, mkdir("deeptools.dir"))
@transform("bowtie2.dir/*.prep.bam",
           regex(r"(.*).bam"),
           r"\1.bam.bai")
def indexPrepBam(infile, outfile):
    '''samtools index bam'''

    statement = '''samtools index -b %(infile)s %(outfile)s'''

    P.run()

    
@follows(indexPrepBam)
@transform("bowtie2.dir/*.prep.bam",
           regex(r"bowtie2.dir/(.*).prep.bam"),
           r"deeptools.dir/\1.coverage.bw")
def bamCoverage(infile, outfile):
    '''Make normalised bigwig tracks with deeptools'''

    job_memory = "2G"
    job_threads = "10"

    if BamTools.isPaired(infile):

        # PE reads filtered on sam flag 66 -> include only first read in properly mapped pairs
        
        statement = '''bamCoverage -b %(infile)s -o %(outfile)s
                    --binSize 5
                    --smoothLength 20
                    --centerReads
                    --normalizeUsingRPKM
                    --samFlagInclude 66
                    -p "max"'''
        
    else:

        # SE reads filtered on sam flag 4 -> exclude unmapped reads
        
        statement = '''bamCoverage -b %(infile)s -o %(outfile)s
                    --binSize 5
                    --smoothLength 20
                    --centerReads
                    --normalizeUsingRPKM
                    --samFlagExclude 4
                    -p "max"'''

    # added smoothLength = 20 to try and get better looking plots...
    # --minMappingQuality 10 optional argument, but unnecessary as bams are alredy filtered
    # centerReads option and small binsize should increase resolution around enriched areas

    print statement
    
    P.run()


@follows(indexPrepBam)
@transform("bowtie2.dir/*.prep.bam",
           regex(r"bowtie2.dir/(.*).prep.bam"),
           r"deeptools.dir/\1.coverage.sizeFilt.bw")
def bamCoverage_sizeFilter(infile, outfile):
    '''Make normalised bigwig tracks with deeptools'''

    job_memory = "2G"
    job_threads = "10"

    if BamTools.isPaired(infile):
        
        statement = '''bamCoverage -b %(infile)s -o %(outfile)s
                    --binSize 5
                    --maxFragmentLength 150
                    --smoothLength 20
                    --centerReads
                    --normalizeUsingRPKM
                    --samFlagInclude 66
                    -p "max"'''
        
    else:
        
        statement = '''echo "Error - BAM must be PE to use --maxFragmentLength parameter" > %(outfile)'''


    print statement
    
    P.run()

    
@follows(bamCoverage, bamCoverage_sizeFilter)
def coverage():
    pass


########################################################
####                    FRIP                        ####
########################################################
def generate_FRIPcountBAM_jobs():

    intervals = glob.glob("macs2.dir/*_peaks.narrowPeak")
    bams = glob.glob("bowtie2.dir/*.prep.bam")
        
    outDir = "BAM_counts.dir/"

    for interval in intervals:
        ifile = [i.split("/")[-1].rstrip("_peaks.narrowPeak") for i in [interval] ]

        for bam in bams:
            bam_sample = bam.split("/")[-1].rstrip(".prep.bam")

            if bam_sample in interval:
                bfile = [b.split("/")[-1][:-len(".prep.bam")] for b in [bam] ]

                bedfile = ' '.join(str(x) for x in ifile )
                bamfile = ' '.join(str(x) for x in bfile )
            
                output = outDir + bedfile + "_fripcounts" + ".txt"
                
                yield ( [ [interval, bam], output ] )
                
    
@follows(mkdir("BAM_counts.dir"), coverage)
@files(generate_FRIPcountBAM_jobs)
def FRIPcountBAM(infiles, outfile):
    '''use bedtools to count reads in bed intervals'''

    interval, bam = infiles

    if BamTools.isPaired(bam):
         # -p flag specifes only to count paired reads

        statement = '''bedtools multicov -p -q 10 -bams %(bam)s 
                    -bed <( awk 'BEGIN {OFS="\\t"} {print $1,$2,$3,$4,$5,$3-$2}' %(interval)s ) 
                    > %(outfile)s 
                    && sed -i '1i \chromosome\\tstart\\tend\\tpeak_id\\tpeak_score\\tpeak_width\\ttotal' 
                    %(outfile)s''' % locals()

    else:

         statement = '''bedtools multicov -q 10 -bams %(bam)s 
                     -bed <( awk 'BEGIN {OFS="\\t"} {print $1,$2,$3,$4,$5,$3-$2}' %(interval)s ) 
                     > %(outfile)s 
                     && sed -i '1i \chromosome\\tstart\\tend\\tpeak_id\\tpeak_score\\tpeak_width\\ttotal' 
                     %(outfile)s''' % locals()

    print statement
         
    P.run()

                 
@transform(FRIPcountBAM, suffix(".txt"), ".load")
def loadFRIPcountBAM(infile, outfile):
    P.load(infile, outfile, options='-i "peak_id"')

    
def generator_BAMtotalcounts():

    bams = glob.glob("bowtie2.dir/*prep.bam")

    outDir = "BAM_counts.dir"

    for bam in bams:
        bfile = bam.split("/")[-1][:-len(".prep.bam")]
        bfile = ''.join(bfile)
        output = outDir + "/" + bfile + "_total_reads.txt"
        
        yield ( [ bam, output ] )

        
@follows(loadFRIPcountBAM)
@files(generator_BAMtotalcounts)
def BAMtotalcounts(infile, outfile):
    '''Count total reads in BAM for normalisation'''

    if BamTools.isPaired(infile):
        statement = '''samtools view -f 2 %(infile)s | wc -l | awk 'BEGIN {OFS="\\t"} {print $0/2}' > %(outfile)s''' % locals()
        # count only reads mapped in proper pairs

    else:
        statement = '''samtools view -F 4 %(infile)s | wc -l  | awk 'BEGIN {OFS="\\t"} {print $0}' > %(outfile)s''' % locals()
        # exclude unmapped reads

    print statement

    P.run()


@follows(mkdir("FRIP.dir/"), BAMtotalcounts)
@transform(FRIPcountBAM,
           regex(r"BAM_counts.dir/(.*)_fripcounts.txt"),
           add_inputs(r"BAM_counts.dir/\1_total_reads.txt"),
           r"FRIP.dir/\1_frip.txt")
def FRIP(infiles, outfile):
    '''Calculate fraction of read in peaks'''

    bam_counts, total_reads = infiles

    db_name = os.path.basename(bam_counts)
    db_name = db_name.replace("-", "_")
    db_name = db_name[:-len(".txt")]
                      
    name = os.path.basename(total_reads)
    name = name[:-len(".txt")]
    
    # read counts to dictionary
    total_reads = open(total_reads, "r").read().replace("\n", "")
    counts = {}
    counts[name] = total_reads
    counts_int = counts[name]
    
    dbhandle = sqlite3.connect(PARAMS["general_database"])
    cc = dbhandle.cursor()
    db = PARAMS["general_database"]
    attach_statement = '''attach %(db)s as db''' % locals()
    cc.execute(attach_statement)
    query =  '''select sum(total) from %(db_name)s''' % locals()
    sql_result = cc.execute(query).fetchall()
    cc.close()

    # sql_result contains a list of 1 tuple (with 1 value),
    # iterate over list and slice tuple
    for x in sql_result:
        result = x[0]
    
    # convert bam counts to int
    counts_int = float(counts_int)
    
    FRIP = (result/counts_int)
    FRIP = str(FRIP)

    table_name = P.snip(name, "_total_reads")
    o = open(outfile, "w")
    columns = [str(x) for x in [table_name, FRIP]]
    o.write("\t".join ( columns ) + "\n")
    o.close()

    
@collate(FRIP,
         regex("FRiP.dir/(.*).txt"),
         r"FRiP.dir/frip_table.txt")
def FRIP_summary(infiles, outfile):
    
    statement = '''cat FRiP.dir/*txt | sed '1i \sample\tFRiP' | tr [[:blank:]] "\\t" > %(outfile)s'''
    print statement
    P.run()

    
@transform(FRIP_summary,
           suffix(".txt"), ".load")
def loadFRIP(infile, outfile):
    P.load(infile, outfile, options = '-i "sample"')


@follows(loadFRIP)
def frip():
    pass


########################################################
####                  Merge Peaks                   ####
########################################################
@follows(frip)
@merge("macs2.dir/*_peaks.filt.bed", "BAM_counts.dir/merged_peaks.bed")
def mergePeaks(infiles, outfile):
    '''cat all peak files, center over peak summit +/- 250 b.p., then merge peaks'''

    infiles = ' '.join(infiles)
    tmp_dir = "$SCRATCH_DIR"
#    window_size = PARAMS[]
    
    statement = '''tmp=`mktemp -p %(tmp_dir)s`; checkpoint;
                   cat %(infiles)s | grep -v ^chrUn* -  > $tmp; checkpoint;
                   awk 'BEGIN {OFS="\\t"} {center = $2 + $10 ; start = center - 250 ; end = center + 250 ;
                     print $1,start,end,$4,$5}' $tmp | 
                   awk 'BEGIN {OFS="\\t"} {if ($2 < $3) print $0}' - |
                   sort -k1,1 -k2,2n |
                   mergeBed -c 4,5 -o count,mean -i - |
                   awk 'BEGIN {OFS="\\t"} {print $1,$2,$3,"merged_peaks_"NR,$5,$4,$3-$2,($2+$3)/2}' - > %(outfile)s; checkpoint;
                   rm $tmp'''

    print statement

    P.run()


########################################################
####                GREAT Peak2Gene                 ####
########################################################
@follows(mergePeaks, mkdir("annotations.dir"))
@files(None,"annotations.dir/ensemblGeneset.txt")
def fetchEnsemblGeneset(infile,outfile):
    ''' Get the *latest* gene records using biomart. The aim here is NOT to match
        the great gene set: For that we would only want protein coding genes with
        GO annotations '''

    statement = '''select gi.gene_id, gi.gene_name,
                          gs.contig, gs.start, gs.end, gs.strand
                   from gene_info gi
                   inner join gene_stats gs
                   on gi.gene_id=gs.gene_id
                   where gi.gene_biotype="protein_coding"
                '''

    anndb = os.path.join(PARAMS["annotations_dir"],
                         "csvdb")

    df = DB.fetch_DataFrame(statement, anndb)
    df.to_csv(outfile, index=False, sep="\t", header=True)

    
@transform(fetchEnsemblGeneset,suffix(".txt"),".load")
def uploadEnsGenes(infile,outfile):
    '''Load the ensembl annotation including placeholder GO ID's'''
    P.load(infile, outfile, options='-i "gene_id" -i "go_id" ')

    
@follows(uploadEnsGenes)
def getGeneLists():
    pass


@follows(getGeneLists, mkdir("greatBeds.dir"))
@files(uploadEnsGenes, "greatBeds.dir/ens_great_prom.bed")
def greatPromoters(infile,outfile):
    ''' Make great promoters for the genes retrieved from Ensembl'''

    dbh = sqlite3.connect(PARAMS["database"])
    cc = dbh.cursor()

    basalup = PARAMS["great_basal_up"]
    basaldown = PARAMS["great_basal_down"]
    maxext = PARAMS["great_max"]
    half = PARAMS["great_half"]  
    statement = '''select distinct contig, start,                                                                                                         
                   end, strand, gene_id from ensemblGeneset '''

    result = cc.execute(statement).fetchall()
    
    
    locations = [ [ str(r[0]), int(r[1]), int(r[2]),str(r[3]), str(r[4]) ] 
                   for r in result ]
    
    writeGreat(locations,basalup,basaldown,maxext,outfile,half)


@transform(greatPromoters,
           regex(r"(.*)_prom.bed"),
           r"\1.bed")
def filterEnsPromoters(infile,outfile):
    '''Remove unwanted chromosomes & correct contig column, "chrchr" -> "chr"'''

    tmp_dir = "$SCRATCH_DIR"
    statement = '''tmp=`mktemp -p %(tmp_dir)s`; 
                sed 's/chrchr/chr/' %(infile)s > $tmp &&
                grep -v ^chrM $tmp > %(outfile)s && rm $tmp'''

    print statement
    
    P.run()

    
@transform(filterEnsPromoters,suffix(".bed"),".load")
def loadGreatPromoters(infile, outfile):
    '''Load the great promoter regions'''
    P.load(infile, outfile, options='-H "chr,start,end,gene_id" -i "gene_id"')

    
@follows(loadGreatPromoters)
def GreatAnnotation():
    pass


@follows(GreatAnnotation, mkdir("regulated_genes.dir"))
@transform("BAM_counts.dir/merged_peaks.bed",
           regex(r"BAM_counts.dir/(.*).bed"),
           add_inputs("greatBeds.dir/ens_great.bed"),
           r"regulated_genes.dir/\1.GREAT.txt")
def regulatedGenes(infiles,outfile):
    '''Get all genes associated with peaks'''

    infile, greatPromoters = infiles

    # intersect infiles with great gene annotation beds to get peak associated genes
    statement = '''intersectBed -wa -wb -a <(cut -f1-7 %(infile)s) -b %(greatPromoters)s 
                | cut -f1-7,11 > %(outfile)s''' % locals()

    # Filter on nearest peak 2 gene later

    print statement

    P.run()

    
@transform(regulatedGenes, suffix(r".txt"), r".load")
def loadRegulatedGenes(infile, outfile):
    P.load(infile, outfile, 
           options='-H "contig,start,end,peak_id,peak_score,no_peaks,peak_width,peak_centre,gene_id" -i "peak_id"')

    
@transform(loadRegulatedGenes,
           suffix(r".load"),
           add_inputs(loadGreatPromoters,uploadEnsGenes),
           r".annotated.bed")
def regulatedTables(infiles, outfile):
    '''Make an informative table about peaks and "regulated" genes'''

    regulated, great, ensGenes = [ P.toTable(x) for x in infiles ]

    query = '''select distinct r.contig,
                  r.start, r.end, r.peak_id, r.peak_score,
                  r.no_peaks, r.peak_width, r.peak_centre,
                  g.gene_id, e.gene_name, e.strand,
                  e.start, e.end
                  from %s as r
                  inner join %s as g
                     on g.gene_id = r.gene_id 
                  inner join %s as e
                     on g.gene_id = e.gene_id
                  ''' % (regulated, great, ensGenes)

    print query
    
    dbh = sqlite3.connect(PARAMS["database"])
    cc = dbh.cursor()
    sqlresult = cc.execute(query).fetchall()

    sql_table = outfile.replace(".bed", ".txt")
    
    o = open(sql_table,"w")
    o.write("\t".join ( 
            ["chromosome","peak_start","peak_end","peak_id","peak_score",
             "no_peaks","peak_width","peak_centre",
             "dist2peak","gene_id", "TSS"]) + "\n" )

    for r in sqlresult:
        contig, pstart, pend, peak_id, peak_score, no_peaks, peak_width, peak_centre = r[0:8]
        gene_id, gene_name, gene_strand, gene_start, gene_end = r[8:13]
        
        if gene_strand == "+": gstrand = 1
        else: gstrand = 2

        tss = getTSS(gene_start,gene_end,gene_strand)

        if gstrand==1: tssdist = tss - ploc
        else: tssdist = ploc - tss

        columns = [ str(x) for x in
                    [  contig, pstart, pend, peak_id, peak_score, peak_width, peak_centre, tssdist, gene_id, tss] ]
        o.write("\t".join( columns  ) + "\n")
    o.close()

    # get closest genes 2 peaks, 1 gene per peak
    tmp_dir = "$SCRATCH_DIR"
    statement = '''tmp=`mktemp -p %(tmp_dir)s`;
                tail -n +2 %(sql_table)s  | sed 's/-//g' 
                | awk 'BEGIN {OFS="\\t"} {print $4,$5,$6,$7,$8,$9,$10,$1,$2,$3}' 
                | sort -k8,8 -k9,9n -k7,7n 
                | cat | uniq -f7 > $tmp 
                && awk 'BEGIN {OFS="\\t"} {print $8,$9,$10,$1,$2,$3,$4,$5,$6,$7}' < $tmp 
                > %(outfile)s  && rm %(sql_table)s $tmp''' % locals()
    print statement

    
@transform(regulatedTables, suffix(".bed"), ".load")
def loadRegulatedTables(infile,outfile):
    P.load(infile,outfile,
           options='-H"contig,peak_start,peak_end,peak_id,peak_score,peak_width,peak_centre,TSSdist,gene_id,TSS" -i "peak_id" ')


@follows(loadRegulatedTables)
def great():
    pass


########################################################
####     Differential Accessibility Read Counts     ####
########################################################
@follows(great)
@transform("bowtie2.dir/*.prep.bam",
           regex(r"(.*).prep.bam"),
           r"\1.prep.bam.bai")
def indexBAM(infile, outfile):
    '''Index input BAM files'''

    statement = '''samtools index %(infile)s %(outfile)s'''

    P.run()

    
def generate_scoreIntervalsBAM_jobs():
    
    # list of bed files & bam files, from which to create jobs
    intervals = glob.glob("regulated_genes.dir/*annotated.bed")
    bams = glob.glob("bowtie2.dir/*.prep.bam")

    outDir = "BAM_counts.dir/"

    for interval in intervals:
        #print interval
        ifile = [i.split("/")[-1][:-len(".annotated.bed")] for i in [interval] ]
        # iterate over intervals generating infiles & partial filenames

        for bam in bams:
            bfile = [b.split("/")[-1][:-len(".prep.bam")] for b in [bam] ]
            # for each interval, iterate over bams generating infiles & partial filenames
            bedfile = ' '.join(str(x) for x in ifile )
            bamfile = ' '.join(str(x) for x in bfile )

            output = outDir + bedfile + "." + bamfile + "_counts.txt"
            # output = outfiles. 1 for each bed/bam combination

            yield ( [ [interval, bam], output ] )

            
@follows(indexBAM)
@files(generate_scoreIntervalsBAM_jobs)
def scoreIntervalsBAM(infiles, outfile):
    '''use bedtools to count reads in bed intervals'''

    interval, bam = infiles

    tmp_dir = "$SCRATCH_DIR"
    
    if BamTools.isPaired(bam):
        statement = '''tmp=`mktemp -p %(tmp_dir)s`;
                       cut -f1-7,9 %(interval)s > $tmp; checkpoint;
                       bedtools multicov -p -q 10 -bams %(bam)s -bed $tmp > %(outfile)s; checkpoint;
                       sed -i '1i \chromosome\\tstart\\tend\\tpeak_id\\tpeak_score\\tpeak_width\\tpeak_center\\tgene_id\\ttotal' %(outfile)s; checkpoint;
                       rm $tmp''' % locals()
        
    else:
         statement = '''tmp=`mktemp -p %(tmp_dir)s`;
                        cut -f1-7,9 %(interval)s > $tmp; checkpoint;
                        bedtools multicov -q 10 -bams %(bam)s -bed $tmp > %(outfile)s; checkpoint; 
                        sed -i '1i \chromosome\\tstart\\tend\\tpeak_id\\tpeak_score\\tpeak_width\\tpeak_center\\tgene_id\\ttotal' %(outfile)s; checkpoint;
                        rm $tmp''' % locals()

    print statement

    P.run()

    
@transform(scoreIntervalsBAM, suffix(".txt"), ".load")
def loadIntervalscoresBAM(infile, outfile):
    P.load(infile, outfile, options='-i "gene_id"')

    
def normaliseBAMcountsGenerator():

    total_reads = glob.glob("BAM_counts.dir/*_total_reads.txt")
    counts = glob.glob("BAM_counts.dir/*counts.txt")

    if len(total_reads)==0:
        yield []
        
    outdir = "BAM_counts.dir/"

    # generate jobs & match total_reads to counts files
    for interval_count in counts:
        count_table = interval_count[:-len("_counts.txt") ] 

        for read_count in total_reads:
            output = count_table + "_norm_counts.txt"

            bam_str = count_table.split(".")[-1]
            if bam_str in read_count:
                
                yield ( [ [interval_count, read_count], output ] )

                
@follows(loadIntervalscoresBAM)
@files(normaliseBAMcountsGenerator)
def normaliseBAMcounts(infiles, outfile):
    '''normalise BAM counts for file size'''
    
#    to_cluster = True
#    job_memory = "5G"
    
    interval_counts, total_reads = infiles
    
    # read counts to dictionary
    name = os.path.basename(total_reads)[:-len(".txt")]
    total_reads = open(total_reads, "r").read().replace("\n", "")
    counts = {}
    counts[name] = total_reads
    counts_sample = counts[name]

    # norm factor = total reads / 1e+06
    norm_factor = float(counts_sample) / 1000000

    sample_name = name.rstrip("_total_reads")

    statement = '''tail -n +2 %(interval_counts)s |
                   awk 'BEGIN {OFS="\\t"} 
                     {if ($3-$2 > 0) width = $3-$2; else width = 1}
                     {print $1,$2,$3,$4,$5,$9,
                       sprintf("%%f", $9/%(norm_factor)s),sprintf("%%f", ($9/%(norm_factor)s)/width),width,"%(sample_name)s"}'
                     - > %(outfile)s'''

    # add 1 if width == 0 to avoid division by 0
    # sprintf - decimal format for normalised counts
    
    print statement

    P.run()

@transform(normaliseBAMcounts, suffix(".txt"), ".load")
def loadnormaliseBAMcounts(infile, outfile):
    P.load(infile, outfile, options='-H "chromosome,start,end,peak_id,peak_score,raw_counts,RPM,RPM_width_norm,peak_width,sample_id" ')    

                
@follows(loadnormaliseBAMcounts)
@merge("BAM_counts.dir/*_norm_counts.txt", "all_norm_counts.txt")
def mergeNormCounts(infiles, outfile):

    infiles = ' '.join(infiles)
    
    statement = '''cat %(infiles)s >  %(outfile)s'''

    P.run()

@transform(mergeNormCounts, suffix(r".txt"), r".load")
def loadmergeNormCounts(infile, outfile):
    P.load(infile, outfile, options='-H "chromosome,start,end,peak_id,peak_score,raw_counts,RPM,RPM_width_norm,peak_width,sample_id" ')

@follows(loadmergeNormCounts)
def count():
    pass


# ---------------------------------------------------
# Generic pipeline tasks
@follows(mapping, peakcalling, coverage, frip, count)
def full():
    pass


@follows(mkdir("report"))
def build_report():
    '''build report from scratch.

    Any existing report will be overwritten.
    '''

    E.info("starting report build process from scratch")
    P.run_report(clean=True)


@follows(mkdir("report"))
def update_report():
    '''update report.

    This will update a report with any changes inside the report
    document or code. Note that updates to the data will not cause
    relevant sections to be updated. Use the cgatreport-clean utility
    first.
    '''

    E.info("updating report")
    P.run_report(clean=False)


@follows(update_report)
def publish_report():
    '''publish report in the CGAT downloads directory.'''

    E.info("publishing report")
    P.publish_report()

if __name__ == "__main__":
    sys.exit(P.main(sys.argv))
