##############################################################################
#
#   MRC FGU CGAT
#
#   $Id$
#
#   Copyright (C) 2009 Andreas Heger
#
#   This program is free software; you can redistribute it and/or
#   modify it under the terms of the GNU General Public License
#   as published by the Free Software Foundation; either version 2
#   of the License, or (at your option) any later version.
#
#   This program is distributed in the hope that it will be useful,
#   but WITHOUT ANY WARRANTY; without even the implied warranty of
#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#   GNU General Public License for more details.
#
#   You should have received a copy of the GNU General Public License
#   along with this program; if not, write to the Free Software
#   Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
###############################################################################
"""===========================
Pipeline template
===========================

:Author: Andreas Heger
:Release: $Id$
:Date: |today|
:Tags: Python

.. Replace the documentation below with your own description of the
   pipeline's purpose

Overview
========

This pipeline computes the word frequencies in the configuration
files :file:``pipeline.ini` and :file:`conf.py`.

Usage
=====

See :ref:`PipelineSettingUp` and :ref:`PipelineRunning` on general
information how to use CGAT pipelines.

Configuration
-------------

The pipeline requires a configured :file:`pipeline.ini` file.
CGATReport report requires a :file:`conf.py` and optionally a
:file:`cgatreport.ini` file (see :ref:`PipelineReporting`).

Default configuration files can be generated by executing:

   python <srcdir>/pipeline_tad.py config

Input files
-----------

None required except the pipeline configuration files.

Requirements
------------

The pipeline requires the results from
:doc:`pipeline_annotations`. Set the configuration variable
:py:data:`annotations_database` and :py:data:`annotations_dir`.

On top of the default CGAT setup, the pipeline requires the following
software to be in the path:

.. Add any additional external requirements such as 3rd party software
   or R modules below:

Requirements:

* samtools >= 1.1

Pipeline output
===============

.. Describe output files of the pipeline here

Glossary
========

.. glossary::


Code
====

"""
from ruffus import *

import sys
import os
import sqlite3
import CGAT.Experiment as E
import CGATPipelines.Pipeline as P
import glob
import CGAT.Database as DB
import pandas as pd
import CGAT.BamTools as BamTools

# load options from the config file
PARAMS = P.getParameters(
    ["%s/pipeline.ini" % os.path.splitext(__file__)[0],
     "../pipeline.ini",
     "pipeline.ini"])

# add configuration values from associated pipelines
#
# 1. pipeline_annotations: any parameters will be added with the
#    prefix "annotations_". The interface will be updated with
#    "annotations_dir" to point to the absolute path names.
PARAMS.update(P.peekParameters(
    PARAMS["annotations_dir"],
    "pipeline_annotations.py",
    on_error_raise=__name__ == "__main__",
    prefix="annotations_",
    update_interface=True))


# if necessary, update the PARAMS dictionary in any modules file.
# e.g.:
#
# import CGATPipelines.PipelineGeneset as PipelineGeneset
# PipelineGeneset.PARAMS = PARAMS
#
# Note that this is a hack and deprecated, better pass all
# parameters that are needed by a function explicitely.

# -----------------------------------------------
# Utility functions
def connect():
    '''utility function to connect to database.

    Use this method to connect to the pipeline database.
    Additional databases can be attached here as well.

    Returns an sqlite3 database handle.
    '''

    dbh = sqlite3.connect(PARAMS["database"])
    statement = '''ATTACH DATABASE '%s' as annotations''' % (
        PARAMS["annotations_database"])
    cc = dbh.cursor()
    cc.execute(statement)
    cc.close()

    return dbh


def upload2csvdb(df, table, db):

        connect = sqlite3.connect(db)

        df.to_sql(table, connect, if_exists="replace", index=False)

        
# ---------------------------------------------------

# Specific pipeline tasks

########################################################
####             Insulator Peaks                    ####
########################################################
@follows(connect)
@merge("data.dir/*CTCF*.bed",
           r"data.dir/insulators.bed")
def makeInsulators(infiles, outfile):
    '''Merge CTCF & Cohesin peaks to form potential insulator peaks'''

    # Criteria for merging:
    # - CTCF/Cohesin peaks common between datasets
    # - CTCF peaks within 51bp of Cohesin peaks
    
    ctcf = ' '.join(infiles)

    rad21 = glob.glob("data.dir/*RAD21*bed")

    tmp_dir = "$SCRATCH_DIR"


    if len(rad21)==0:
        
        statement = '''cat %(ctcf)s | 
                         sort -k1,1 -k2,2n - |
                         mergeBed -c 4,5 -o collapse,mean -d 51 -i - |
                         awk 'BEGIN {OFS="\\t"} {if ($4 ~ ",") print $0}' - > %(outfile)s'''

    if len(rad21)==1:
        
        rad21 = ' '.join(rad21)
       
        statement = '''ctcf=`mktemp -p %(tmp_dir)s`; checkpoint;
                       cat %(ctcf)s | 
                         sort -k1,1 -k2,2n - |
                         mergeBed -c 4,5 -o collapse,mean -d 51 -i - |
                         awk 'BEGIN {OFS="\\t"} {if ($4 ~ ",") print $0}' - > $ctcf ; checkpoint;
                       cat $ctcf <( cut -f1-5 %(rad21)s ) |
                         sort -k1,1 -k2,2n - |
                         mergeBed -c 4,5 -o collapse,mean -d 51 -i - > %(outfile)s'''
                  
    else:

       rad21 = ' '.join(rad21)
       
       statement = '''ctcf=`mktemp -p %(tmp_dir)s`; checkpoint;
                       rad21=`mktemp -p %(tmp_dir)s`; checkpoint;
                       cat %(ctcf)s | 
                         sort -k1,1 -k2,2n - |
                         mergeBed -c 4,5 -o collapse,mean -d 51 -i - |
                         awk 'BEGIN {OFS="\\t"} {if ($4 ~ ",") print $0}' - > $ctcf ; checkpoint;
                       cat %(rad21)s | 
                         sort -k1,1 -k2,2n - |
                         mergeBed -c 4,5 -o collapse,mean -d 51 -i - |
                         awk 'BEGIN {OFS="\\t"} {if ($4 ~ ",") print $0}' - > $rad21 ; checkpoint;
                       cat $ctcf $rad21 |
                         sort -k1,1 -k2,2n - |
                         mergeBed -c 4,5 -o collapse,mean -d 51 -i - > %(outfile)s'''

    print statement

    P.run()

    
@follows(makeInsulators)
@transform("data.dir/insulators.bed",
           regex(r"(.*).bed"),
           r"\1.load")
def loadPeaks(infile, outfile):
    '''load peak data to db'''
    P.load(infile, outfile, options='-H "contig,start,end,peak_id,peak_score" ')

########################################################
####               ChIP Coverage                    ####
########################################################
@follows(loadPeaks)
@transform("data.dir/*.bam",
           regex(r"(.*).bam"),
           r"\1.bam.bai")
def indexBAM(infile, outfile):
    '''Index input BAM files'''

    statement = '''samtools index %(infile)s %(outfile)s'''

    P.run()

def generate_scoreIntervalsBAM_jobs():

    # list of bed files & bam files, from which to create jobs
    intervals = glob.glob("data.dir/insulators.bed")
    bams = glob.glob("data.dir/*.bam")

    outDir = "BAM_counts.dir/"

    for interval in intervals:
        ifile = [i.split("/")[-1][:-len(".bed")] for i in [interval] ]
        # iterate over intervals generating infiles & partial filenames

        for bam in bams:
            bfile = [b.split("/")[-1][:-len(".bam")] for b in [bam] ]
            # for each interval, iterate over bams generating infiles & partial filenames
            bedfile = ' '.join(str(x) for x in ifile )
            bamfile = ' '.join(str(x) for x in bfile )

            output = outDir + bedfile + "." + bamfile + "_counts.txt"
            # output = outfiles. 1 for each bed/bam combination

            yield ( [ [interval, bam], output ] )

@follows(indexBAM, mkdir("BAM_counts.dir/"))
@files(generate_scoreIntervalsBAM_jobs)
def scoreIntervalsBAM(infiles, outfile):
    '''use bedtools to count reads in bed intervals'''

    interval, bam = infiles

    tmp_dir = "$SCRATCH_DIR"
    
    if BamTools.isPaired(bam):
        options = "-p"

    else:
        options = " "
        
    statement = '''tmp=`mktemp -p %(tmp_dir)s`;checkpoint ;
                   awk 'BEGIN {OFS="\\t"} 
                     {pcenter=($2+$3)/2} 
                     {print $1,sprintf("%%d", pcenter-100),sprintf("%%d", pcenter+100),$4,$5}' 
                     %(interval)s > $tmp; checkpoint;
                   bedtools multicov
                     %(options)s
                     -q 10 
                     -bams %(bam)s 
                     -bed $tmp
                     > %(outfile)s 
                    && sed 
                     -i '1i \chromosome\\tstart\\tend\\tpeak_id\\tpeak_score\\ttotal' 
                     %(outfile)s''' 
        
    print statement

    P.run()

@transform(scoreIntervalsBAM, suffix(".txt"), ".load")
def loadIntervalscoresBAM(infile, outfile):
    P.load(infile, outfile, options='-i "gene_id"')

def generator_BAMtotalcounts():
    bams = glob.glob("data.dir/*.bam")
    outDir = "BAM_counts.dir/"

    for bam in bams: 
        bfile = [b.split("/")[-1][:-len(".bam")] for b in [bam] ]
        bfile = ' '.join(str(x) for x in bfile ) # unpack from list
        output = outDir + bfile + "_total_reads.txt"
        
        yield ( [ bam, output ] )
        
@follows(loadIntervalscoresBAM)
@files(generator_BAMtotalcounts)
def BAMtotalcounts(infile, outfile):
    '''Count total reads in BAM for normalisation'''

    if BamTools.isPaired(infile):
        # count only reads mapped in proper pairs
        statement = '''samtools view -f 2 %(infile)s | wc -l | 
                         awk 'BEGIN {OFS="\\t"} {print $0/2}' > %(outfile)s'''

    else:
        # exclude unmapped reads
        statement = '''samtools view -F 4 %(infile)s | wc -l  | 
                         awk 'BEGIN {OFS="\\t"} {print $0}' > %(outfile)s''' 


    print statement

    P.run()

@transform(BAMtotalcounts, suffix(".txt"), r".load")
def loadBAMtotalcounts(infile, outfile):
    P.load(infile, outfile, options='-H "total_reads"')

def normaliseBAMcountsGenerator():

    total_reads = glob.glob("BAM_counts.dir/*_total_reads.txt")
    counts = glob.glob("BAM_counts.dir/*counts.txt")

    if len(total_reads)==0:
        yield []
        
    outdir = "BAM_counts.dir/"

    # generate jobs & match total_reads to counts files
    for interval_count in counts:
        count_table = interval_count[:-len("_counts.txt") ] 

        for read_count in total_reads:
            output = count_table + "_norm_counts.txt"

            bam_str = count_table.split(".")[-1]
            if bam_str in read_count:
                
                yield ( [ [interval_count, read_count], output ] )

@follows(loadBAMtotalcounts)
@files(normaliseBAMcountsGenerator)
def normaliseBAMcounts(infiles, outfile):
    '''normalise BAM counts for file size'''
    
#    to_cluster = True
#    job_memory = "5G"
    
    interval_counts, total_reads = infiles
    
    # read counts to dictionary
    name = os.path.basename(total_reads)[:-len(".txt")]
    total_reads = open(total_reads, "r").read().replace("\n", "")
    counts = {}
    counts[name] = total_reads
    counts_sample = counts[name]

    # norm factor = total reads / 1e+06
    norm_factor = float(counts_sample) / 1000000

    sample_name = name.rstrip("_total_reads")

    statement = '''tail -n +2 %(interval_counts)s |
                   awk 'BEGIN {OFS="\\t"} 
                     {if ($3-$2 > 0) width = $3-$2; else width = 1}
                     {print $1,$2,$3,$4,$5,$8,
                       sprintf("%%f", $8/%(norm_factor)s),sprintf("%%f", ($8/%(norm_factor)s)/width),width,"%(sample_name)s"}'
                     - > %(outfile)s'''

    # add 1 if width == 0 to avoid division by 0
    # sprintf - decimal format for normalised counts
    
    print statement

    P.run()

# @transform(normaliseBAMcounts, suffix(".txt"), ".load")
# def loadnormaliseBAMcounts(infile, outfile):
#     P.load(infile, outfile, options='-H "chromosome,start,end,peak_id,peak_score,raw_counts,RPM,RPM_width_norm,peak_width,sample_id" ')    
              
    
@follows(normaliseBAMcounts)
@merge("BAM_counts.dir/*_norm_counts.txt", "all_norm_counts.txt")
def mergeNormCounts(infiles, outfile):

    infiles = ' '.join(infiles)
    
    statement = '''cat %(infiles)s >  %(outfile)s'''

    P.run()

@transform(mergeNormCounts, suffix(r".txt"), r".load")
def loadmergeNormCounts(infile, outfile):
    P.load(infile, outfile, options='-H "chromosome,start,end,peak_id,peak_score,raw_counts,RPM,RPM_width_norm,peak_width,sample_id" ')


########################################################
####               Motif Analysis                   ####
########################################################
@transform(loadPeaks,
           regex(r"(.*).load"),
           r"\1_meme.bed")
def addPeakInfo(infile, outfile):
    '''Make bed of peak_centers, sorted by peak score'''

    table = os.path.basename(infile)[:-len(".load")].replace(".", "_")
    
    query = '''SELECT contig, start, end, peak_id, peak_score FROM %(table)s''' % locals()

    dbh = sqlite3.connect(PARAMS["database"])
    cc = dbh.cursor()
    sqlresult = cc.execute(query).fetchall()

    o = open(outfile, "w")
    
    for r in sqlresult:
        contig, start, end, peak_id, peak_score = r[0:5]

        peak_center = (start + end)/ 2
        peak_center_minus1 = (peak_center - 1 )
        peak_center_plus1 = (peak_center + 1 )

        columns = [ str(x) for x in
                    [ contig, peak_center_minus1, peak_center_plus1, peak_id, peak_score ] ]

        o.write("\t".join ( columns ) + "\n")
    o.close()

    tmp = outfile +"_tmp"
    
    statement = '''sort -k5,5nr %(outfile)s > %(tmp)s && mv %(tmp)s %(outfile)s''' % locals()

    ### Change this function to include CTCF & cohesin coverage!
    
    P.run()


def getMemeForegroundBedGenerator():
    
    beds = glob.glob("data.dir/*_meme.bed")

    if len(beds) == 0:
        yield []

    npeaks = [x.strip() for x in str(PARAMS["meme_npeaks"]).split(",")]
    widths = [y.strip() for y in str(PARAMS["meme_widths"]).split(",")]

    outdir = "meme.seq.dir/"
    
    for bed in beds:
        int_name = os.path.basename(bed)[:-len("_meme.bed")]
        for n in npeaks:
            for w in widths:
                fgname = ".".join([int_name,n,w,"foreground","bed"])
                outfile = outdir + fgname
                
                yield [ bed, outfile ]

                
@follows(addPeakInfo, mkdir("meme.seq.dir/"))
@files(getMemeForegroundBedGenerator)
def getMemeForegroundBed(infile, outfile):
    '''Make meme beds of peak center +/- n bp, for n top scoring peaks'''

    chrom_sizes = PARAMS["annotations_chrom_sizes"]
    
    npeaks = os.path.basename(outfile)[:-len(".foreground.bed")].split(".")[-2]
    width = os.path.basename(outfile)[:-len(".foreground.bed")].split(".")[-1]
    start_width = (int(width) / 2) + 1 # correct for previous offset
    end_width = (int(width) / 2) - 1 # as bed file is chr, peak_centre_minus1, peak_centre_plus1
    
    if npeaks != "all":
        statement = '''slopBed -i <(  head -n %(npeaks)s %(infile)s )
                    -g %(chrom_sizes)s -l %(start_width)s -r %(end_width)s -s > %(outfile)s''' % locals()
        
    else:
        statement = '''slopBed -i  %(infile)s 
                    -g %(chrom_sizes)s -l %(start_width)s -r %(end_width)s -s > %(outfile)s''' % locals()
   
    P.run()

    
@transform(getMemeForegroundBed, suffix(".bed"), r".load")
def loadMemeForegroundBed(infile, outfile):
    P.load(infile, outfile, options='-H "contig,start,end,peak_id,TFscore" ')
    
@follows(loadMemeForegroundBed)
@transform(getMemeForegroundBed,
           regex(r"(.*).foreground.bed"),
           r"\1.background.bed")
def getMemeBackgroundBed(infile, outfile):
    '''get bed file of peak flanking regions (of equal width to peak) for meme background model'''

    genome_idx = os.path.join(PARAMS["annotations_mm10dir"],"assembly.dir/contigs.tsv")
    
    statement ='''sort -k1,1 -k2,2n %(infile)s 
               | bedtools flank -pct -l 1 -r 1 -g %(genome_idx)s > %(outfile)s'''
               # -pct -l 1 -r 1 will create flanking regions = peak width
    
    P.run()

    
@follows(getMemeBackgroundBed)
@transform("meme.seq.dir/*.bed",
           regex(r"(.*).bed"),
           r"\1.fasta")
def getMemeSequences(infile, outfile):
    '''get the peak sequences, masking or not specificed in the ini file.'''
  
    genome_fasta = os.path.join(PARAMS["genome_dir"],PARAMS["genome"]+".fasta")
    genome_idx = os.path.join(PARAMS["annotations_mm10dir"],"assembly.dir/contigs.tsv")
    mask = PARAMS["meme_mask"]
    
    if mask != "none":
        mask_opt = "-m " + mask
    else:
        mask_opt = ""
        
    statement = '''cat %(infile)s | python ~/devel/cgat/scripts/bed2fasta.py %(mask_opt)s -g %(genome_fasta)s 
                | grep -v "#" > %(outfile)s'''

    P.run()

    
@follows(getMemeSequences)
@transform("meme.seq.dir/*background.fasta", suffix(".fasta"), ".bfile")
def getMemeBfiles(infile, outfile):
    '''prepare the meme background model'''

    statement='''fasta-get-markov -m 2 %(infile)s  > %(outfile)s''' % locals()

    P.run()

    
@follows(getMemeBfiles)
@files(None, "motifs.txt")
def filterTFDatabases(infile, outfile):
    '''Filter TF databases for interesting motifs identified by meme-chip
       & s0pecified in pipeline.ini'''

    TFdb = PARAMS["mast_motif_db"]
    TFdb = TFdb.replace(",", " ")
    motifs = PARAMS["mast_motifs"].split(",")

    header = '''"MEME version 4 \\n\\nALPHABET= ACGT \\n\\nstrands: + - \\n\\nBackground letter frequencies (from uniform background): \\n\\nA 0.2500 C 0.2500 G 0.2500 T 0.2500\\n\\n"'''

    n = 0
    for TF in motifs:
        n = n + 1
        
        if n == 1:

            statement = '''printf %(header)s > %(outfile)s && 
                           awk '/^MOTIF/ {p=0} /%(TF)s/ {p=1} p' <( cat %(TFdb)s ) >> %(outfile)s'''

        else:

            statement = '''awk '/^MOTIF/ {p=0} /%(TF)s/ {p=1} p' <( cat %(TFdb)s ) >> %(outfile)s'''
            
        print statement
        P.run()

        
@follows(filterTFDatabases, mkdir("fimo.dir"))
@transform("meme.seq.dir/*.foreground.fasta",
           regex(r"meme.seq.dir/(.*).foreground.fasta"),
           add_inputs(r"motifs.txt"),
           r"fimo.dir/\1.fimo.log")
def runFimo(infiles, outfile):
    '''Run Fimo to search for CTCF motifs'''

    foreground, motifs = infiles
    
    background = foreground.replace("foreground.fasta", "background.bfile")

    outdir = outfile.split("/")[0] + "/" + outfile.split("/")[-1].rstrip(".fimo.log")
    
    statement = '''mkdir %(outdir)s; checkpoint;
                   fimo 
                     --oc %(outdir)s
                     --bgfile %(background)s
                     --qv-thresh
                     --max-strand
                     %(motifs)s
                     %(foreground)s
                     &> %(outfile)s'''

    print statement

    P.run()

        
@follows(runFimo)
@transform("fimo.dir/*/*txt",
           regex(r"fimo.dir/(.*)/fimo.txt"),
                 r"fimo.dir/\1/\1_fimo_table.txt")
def summarizeFimo(infile, outfile):
    '''summarize fimo results'''
    
    statement = '''cat %(infile)s | grep -v "#" | sort -k1,1n > %(outfile)s'''

    P.run()

    
@transform(summarizeFimo, suffix(".txt"), ".load")
def loadFimo(infile, outfile):

    P.load(infile, outfile,
           options='-H "pattern_name,sequence_name,start,stop,strand,score,p_value,q_value,matched_sequence" ')


@follows(mkdir("meme.chip.dir"))
@transform(getMemeSequences,
           regex(r"meme.seq.dir/(.*).foreground.fasta"),
           r"meme.chip.dir/\1.memechip")
def runMemeChIP(infile, outfile):
    '''run MemeChIP'''

    outdir = outfile.replace(".memechip", "")
    bfile= infile.replace(".foreground.fasta", ".background.bfile")

    motifDb =  " -db ".join(P.asList(PARAMS["meme_motif_db"])) # Meme-Chip needs eac db in list to have "-db" flag
    nmotifs = PARAMS["meme_nmotif"]
    meme_max_jobs = PARAMS["meme_meme_maxsize"]
    
    # nmeme - The upper bound on the number of sequences that are passed to MEME.
    # This is required because MEME takes too long to run for very large sequence sets.
    # All input sequences are passed to MEME if there are not more than limit.
    # default nmeme = 600

    # ccut - The maximum length of a sequence to use before it is trimmed to a central region of this size.
    # A value of 0 indicates that sequences should not be trimmed.

    # meme-maxsize - Change the largest allowed dataset to be size.
    # default meme-maxsize is 100,000.
    # Fine with the default settings for -nmeme (600) and -ccut (100), largest possible dataset size would be 60000.

    # meme-maxsize 10x10^6 - this is far to large, runs take >24hrs
    # will try 600,000, equivalent to max of 600 1000bp seq
    # in order to check 2,000 <= 1,000bp seq will need meme-maxsize of 2x10^6
    
    job_memory ="5G"
    job_threads = "2"

    statement = '''meme-chip
                   -oc %(outdir)s
                   -db %(motifDb)s
                   -bfile %(bfile)s
                   -ccut 0
                   -meme-mod zoops
                   -meme-minw 5
                   -meme-maxw 30
                   -meme-nmotifs %(nmotifs)s
                   -meme-maxsize %(meme_max_jobs)s

                   %(infile)s
                   > %(outfile)s
                ''' % locals()

    print statement

    P.run()

    
########################################################
####                Define TADS                     ####
########################################################
@follows(loadFimo)
@files(None, "TADS.bed")
def defineTads(infile, outfile):
    '''
    Motif is "forward" if present on + strand, and "reverse" if present on - strand
    insulator pairs (from intersection) can therefore have motifs in the following orientations:
    1) convergent (F, R)
    2) divergent (R, F)
    3) same direction + strand (F, F)
    4) same direction - strand (R, R)
    
    Intervals generated from peak intersections with motifs in convergent orientation will represent TADs (or subTADS...)

    '''

    db = PARAMS["database"]
    npeaks = PARAMS["tads_npeaks"]
    pwidth = PARAMS["tads_pwidth"]
    tmp_dir = "$SCRATCH_DIR"
    
    # fetch insulator peaks with fimo motifs
    table = "insulators_" + '_'.join([str(npeaks), str(pwidth)]) + "_fimo_table"
    statement = '''select * from %(table)s''' % locals()
    motifs = DB.fetch_DataFrame(statement, db) 

    # get most significant motif for each peak
    motifs = motifs.sort_values(["sequence_name", "q_value"], 0).drop_duplicates(subset="sequence_name", keep="first")
    
    motifs.to_csv("insulators_fimoMotifs.txt", sep="\t", header=True) # save peaks w/ annotated motifs as df
    upload2csvdb(motifs, "insulators_fimoMotifs", db) # upload to csvdb

    
    # get peaks (bed format) corresponding to fimo motifs
    statement = '''select b.contig, b.start, b.end, a.sequence_name, b.peak_score, a.strand, a.q_value
                   from insulators_fimoMotifs a inner join insulators b on a.sequence_name = b.peak_id'''
    motif_bed = DB.fetch_DataFrame(statement, db)
    motif_bed = motif_bed.sort_values(["sequence_name", "q_value"], 0).drop_duplicates(subset="sequence_name", keep="first")
    motif_bed.to_csv("motif_bed.txt", sep="\t", header=True, index=False)

    # merge peaks
    # iterate over a range of distances (1mb - 1kb) within which insulators peaks are merged
             ### descending order of distances -> favours bigger TADs, and joins up remaining intervals up to min. size of 1kb
    # merged insulators selected for with awk "," in $6 (strand col) ***Limited to merges of two peaks with F,R orientation with ($6=="\+,-")
    # merged insulators written to tmp file (tmp + str(counter))
    # for each successive merge after n=1 peaks from previous merge are subtracted from results with bedtools (w/ -A flag to remove entire intervals)
             ### a few of the later TADs are ver large and are merged over previous, how to correct for this? merge final file (only overlapping tads?)

    # n = 0

    # distances = range(0, 10100000, 10000) # 10mb to 1kb, 10kb decreases
    # distances = distances[::-1] # invert list -> descending order

    # for dist in distances:
    #     n = n +1
    #     tmp = "tmp" + str(n)
        
    #     if n == 1:
    #         statement = '''tmp=`mktemp -p %(tmp_dir)s`; checkpoint;
    #                        tail -n+2 motif_bed.txt | 
    #                          sort -k1,1 -k2,2n - > $tmp; checkpoint;
    #                        mergeBed -c 4,5,6,7 -o collapse,mean,collapse,mean -d %(dist)s -i $tmp |
    #                          awk 'BEGIN {OFS="\\t"} {if ($6 == "\+,-") print $0}' - > %(tmp)s''' % locals()

    #     elif n > 1 and n < len(distances):
    #         merge = tmp.replace(str(n), str(n-1))
    #         statement = '''tmp=`mktemp -p %(tmp_dir)s`; checkpoint;
    #                        tail -n+2 motif_bed.txt | 
    #                          sort -k1,1 -k2,2n - |
    #                        mergeBed -c 4,5,6,7 -o collapse,mean,collapse,mean -d %(dist)s -i - |
    #                          awk 'BEGIN {OFS="\\t"} {if ($6 == "\+,-") print $0}' - > $tmp; checkpoint;
    #                        subtractBed -A -a $tmp -b %(merge)s > %(tmp)s''' % locals()

    #     elif n == len(distances):
    #         merge = tmp.replace(str(n), str(n-1))
    #         statement = '''tmp=`mktemp -p %(tmp_dir)s`; checkpoint;
    #                        tail -n+2 motif_bed.txt | 
    #                          sort -k1,1 -k2,2n - |
    #                        mergeBed -c 4,5,6,7 -o collapse,mean,collapse,mean -d %(dist)s -i - |
    #                          awk 'BEGIN {OFS="\\t"} {if ($6 == "\+,-") print $0}' - > $tmp; checkpoint;
    #                        subtractBed -A -a $tmp -b %(merge)s > %(tmp)s; checkpoint;
    #                        awk 'BEGIN {OFS="\\t"} {if ($3-$2 > 1000) print $0}' <(cat tmp*) |
    #                          sort -k1,1 -k2,2n - | 
    #                          mergeBed -c 4,5,6,7 -o collapse,mean,collapse,mean -i - > %(outfile)s; checkpoint;
    #                        rm tmp*''' % locals()

        
    ### Instead of merging peaks with F/R motif orientation I could seperate insulator peaks into F & R files,
    ### then use bedtools closest to intersect peaks up to a max distance of n & remove peaks with divergent motifs.

    # Ensure "closest" matches are on the same chromosome with awk, also remove features > 1mb wide
    
    statement = '''tmp=`mktemp -p %(tmp_dir)s`; checkpoint;
                   Fstrand=`mktemp -p %(tmp_dir)s`; checkpoint;
                   Rstrand=`mktemp -p %(tmp_dir)s`; checkpoint;
                   awk 'BEGIN {OFS="\\t"} {if ($6 == "+") print $0}' <(tail -n+2 motif_bed.txt | sort -k1,1 -k2,2n ) > $Fstrand; checkpoint;
                   awk 'BEGIN {OFS="\\t"} {if ($6 == "-") print $0}' <(tail -n+2 motif_bed.txt | sort -k1,1 -k2,2n ) > $Rstrand; checkpoint;
                   ~/devel/GIT/bedtools2/bin/closestBed -iu -D ref 
                         -a $Fstrand 
                         -b $Rstrand
                         > $tmp; checkpoint; 
                   awk 'BEGIN {OFS="\\t"} {if ($1 == $8 && $9-$2 < 1000000) print $1,$2,$9,$4"/"$11,($5+$12)/2,$6","$13,($7+$14)/2}' $tmp > %(outfile)s '''

                   ### This works better!

    # Need to incoporate CTCF & cohesin coverage over candidate insulators. Then filter out insulator pairs (candidate TADS) with large discrepancies in ChIP signal
    # Czimmerer et al use a cut off of > 2fold difference betweeen start & end peaks of TADS in ChIP signal
    # Add ChIP coverage code for insulator peaks & save to db, then incoporate CTCF & cohesin signal into awk filter at the end of this statement
    
    print statement
    P.run()


########################################################
####              Coverage tracks                   ####
########################################################
@follows(defineTads, mkdir("macs2.dir"))
@transform("data.dir/*prep.bam",
           regex(r"data.dir/(.*)-sample.*.prep.bam"),
           r"macs2.dir/\1.Macs2SPMR.log")
def SPMRWithMACS2(infile, outfile):
    '''Calculate signal per million reads with MACS2, output bedGraph'''

    # --SPMR ask MACS2 to generate pileup signal file of 'fragment pileup per million reads'
    
    sample = infile
    WCE = sample.replace("-sample", "-WCE")
    
    name = P.snip(outfile, ".Macs2SPMR.log").split("/")[-1]
    fragment_size = PARAMS["macs2_fragment_size"]

    job_memory = "10G"
    
    if BamTools.isPaired(sample):
        statement = '''macs2 callpeak 
        --format=BAMPE
        --treatment %(sample)s
        --verbose=10
        --name=%(name)s
        --outdir=macs2.dir
        --qvalue=0.1
        --bdg
        --SPMR
        --control %(WCE)s
        --mfold 5 50
        --gsize 1.87e9
        >& %(outfile)s''' % locals()

    else:
        statement = '''macs2 callpeak 
        --format=BAM
        --treatment %(sample)s
        --verbose=10
        --name=%(name)s
        --outdir=macs2.dir
        --qvalue=0.1
        --bdg
        --SPMR
        --control %(WCE)s
        --tsize %(fragment_size)s
        --mfold 5 50
        --gsize 1.87e9
        >& %(outfile)s''' % locals()
            
    print statement
    P.run()


@transform(SPMRWithMACS2,
       regex(r"macs2.dir/(.*).Macs2SPMR.log"),
       r"macs2.dir/\1.fold_enrichment.bdg")
def BDGCompWithMACS2(infile, outfile):
        '''run MACS 2 for bedgraph comparison. Output files contain linear scale fold enrichments (ChIP/WCE).'''

        job_memory = "10G"
        jop_threads = "2"
        
        sample = infile[:-len("_Macs2SPMR.log")] + "_treat_pileup.bdg"
        control = sample.replace("treat_pileup", "control_lambda")
        
        statement = '''macs2 bdgcmp -t %(sample)s -c %(control)s -m FE -o %(outfile)s''' % locals()

        print statement
        
        P.run()

        
@transform(BDGCompWithMACS2,
           regex(r"macs2.dir/(.*).fold_enrichment.bdg"),
           r"macs2.dir/\1.sort.bdg")
def sort_bdg(infile,outfile):
    '''sort bedGraph'''

    # Include sed filter step if some chr names cause errors when converting to BigWig
    statement = '''sort -k1,1 -k2,2n <(sed '/^chrY_JH584300_random/d' %(infile)s) > %(outfile)s'''
    
    P.run()

@follows(mkdir("deepTools"))
@transform(sort_bdg,
           regex(r"macs2.dir/(.*).sort.bdg"),
           r"deepTools/\1.bw")
def BDGtoBigWig(infile, outfile):
    '''Convert .bdg MACS2 output to .bw'''

    chrom_size = PARAMS["annotations_chrom_sizes"]
    
    statement = '''bedGraphToBigWig %(infile)s %(chrom_size)s %(outfile)s'''

    print statement

    P.run()


@follows(BDGtoBigWig)
@transform("deepTools/*-1.bw",
           regex(r"(.*)-1.bw"),
           r"\1.merge.bdg")
def mergeBWreps(infile, outfile):
    '''merge replicates to one BW for IGV visualisation'''

    bw1 = infile
    bw2 = bw1.replace("-1.bw", "-2.bw")

    tmp_dir = "$SCRATCH_DIR"
    
    statement = '''tmp=`mktemp -p %(tmp_dir)s`; checkpoint;
                   bigWigMerge %(bw1)s %(bw2)s $tmp; checkpoint;
                   sort -k1,1 -k2,2n $tmp > %(outfile)s'''

    print statement

    P.run()

@transform(mergeBWreps, suffix(r".bdg"), r".bw")
def BW2BDG(infile, outfile):
    '''convert bedgraph back to bigwig'''

    chrom_sizes = PARAMS["annotations_chrom_sizes"]
    
    statement = '''bedGraphToBigWig %(infile)s %(chrom_sizes)s %(outfile)s'''

    print statement

    P.run()

    
    
@follows(BW2BDG)
def macs2BW():
    pass

# ---------------------------------------------------
# Generic pipeline tasks
@follows(macs2BW)
def full():
    pass


@follows(mkdir("report"))
def build_report():
    '''build report from scratch.

    Any existing report will be overwritten.
    '''

    E.info("starting report build process from scratch")
    P.run_report(clean=True)


@follows(mkdir("report"))
def update_report():
    '''update report.

    This will update a report with any changes inside the report
    document or code. Note that updates to the data will not cause
    relevant sections to be updated. Use the cgatreport-clean utility
    first.
    '''

    E.info("updating report")
    P.run_report(clean=False)


@follows(update_report)
def publish_report():
    '''publish report in the CGAT downloads directory.'''

    E.info("publishing report")
    P.publish_report()

if __name__ == "__main__":
    sys.exit(P.main(sys.argv))
