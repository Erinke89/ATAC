"""===========================
Pipeline template
===========================

.. Replace the documentation below with your own description of the
   pipeline's purpose

Overview
========

This pipeline computes the word frequencies in the configuration
files :file:``pipeline.ini` and :file:`conf.py`.

Usage
=====

See :ref:`PipelineSettingUp` and :ref:`PipelineRunning` on general
information how to use CGAT pipelines.

Configuration
-------------

The pipeline requires a configured :file:`pipeline.ini` file.
CGATReport report requires a :file:`conf.py` and optionally a
:file:`cgatreport.ini` file (see :ref:`PipelineReporting`).

Default configuration files can be generated by executing:

   python <srcdir>/pipeline_footprint.py config

Input files
-----------

None required except the pipeline configuration files.

Requirements
------------

The pipeline requires the results from
:doc:`pipeline_annotations`. Set the configuration variable
:py:data:`annotations_database` and :py:data:`annotations_dir`.

On top of the default CGAT setup, the pipeline requires the following
software to be in the path:

.. Add any additional external requirements such as 3rd party software
   or R modules below:

Requirements:

* samtools >= 1.1

Pipeline output
===============

.. Describe output files of the pipeline here

Glossary
========

.. glossary::


Code
====

"""
from ruffus import *

import sys
import os
import sqlite3
import CGAT.Experiment as E
import CGATPipelines.Pipeline as P
import CGAT.Database as DB
import glob
import pandas as pd

# load options from the config file
PARAMS = P.getParameters(
    ["%s/pipeline.ini" % os.path.splitext(__file__)[0],
     "../pipeline.ini",
     "pipeline.ini"])

# add configuration values from associated pipelines
#
# 1. pipeline_annotations: any parameters will be added with the
#    prefix "annotations_". The interface will be updated with
#    "annotations_dir" to point to the absolute path names.
PARAMS.update(P.peekParameters(
    PARAMS["annotations_dir"],
    "pipeline_genesets.py",
    on_error_raise=__name__ == "__main__",
    prefix="annotations_",
    update_interface=True))


# if necessary, update the PARAMS dictionary in any modules file.
# e.g.:
#
# import CGATPipelines.PipelineGeneset as PipelineGeneset
# PipelineGeneset.PARAMS = PARAMS
#
# Note that this is a hack and deprecated, better pass all
# parameters that are needed by a function explicitely.

# -----------------------------------------------
# Utility functions
def connect():
    '''utility function to connect to database.

    Use this method to connect to the pipeline database.
    Additional databases can be attached here as well.

    Returns an sqlite3 database handle.
    '''

    dbh = sqlite3.connect(PARAMS["database_name"])
    statement = '''ATTACH DATABASE '%s' as annotations''' % (
        PARAMS["annotations_database"])
    cc = dbh.cursor()
    cc.execute(statement)
    cc.close()

    return dbh


# ---------------------------------------------------
# Specific pipeline tasks
@collate("data.dir/*.bam",
       regex(r"(.*)_r[1-9].bam"),
       r"\1_merge.bam")
def mergeBams(infiles, outfile):
    '''merge bams into single file per experimental condition'''

    infiles = ' '.join(infiles)
    job_threads = 5

    statement = '''samtools cat 
                     %(infiles)s |
                   samtools sort - 
                     > %(outfile)s'''
    
    P.run()

    
@transform(mergeBams,
           suffix(r".bam"),
           r".bam.bai")
def indexBams(infile, outfile):
    '''index merged bams'''

    job_threads = 5
    statement = '''samtools index -b %(infile)s %(outfile)s'''
    
    P.run()


@follows(indexBams, mkdir("cutsites.dir"))
@transform(mergeBams,
           regex(r"data.dir/(.*)_merge.bam"),
           r"cutsites.dir/\1.cutsites.bed.gz")
def getCutSites(infile, outfile):
    '''Seperate forward and reverse strands, 
       offset read start position (F + 4, R - 5),
       and merge back to 1 file'''

    job_memory = "6000M"
    tsv = outfile.replace(".gz", "")
        
    statement = '''tmp=`mktemp -p %(local_tmpdir)s`; checkpoint;
                   samtools sort -n %(infile)s > $tmp; checkpoint;
                   bedtools bamtobed -i $tmp |
                   awk 'BEGIN {OFS="\\t"} 
                     {pshift = $2+4} 
                     {nshift = $2-5} 
                     {if ($6 = "+") print $1,pshift,pshift+1 ;
                     else if ($6 = "-") print $1,nshift,nshift-1 }' -
                     > %(tsv)s ; checkpoint;
                   gzip %(tsv)s ; checkpoint;
                   rm $tmp'''

    P.run()

    
@follows(getCutSites)
@transform("data.dir/*.bed",
          regex(r"data.dir/(.*).bed"),
          r"coverage.dir/\1.window.bed")
def offsetPeaks(infile, outfile):
    '''Offset peaks to peak centre +/- n b.p.
       And remove peaks from chr*random, chrun*, chrM'''
    
    summits = PARAMS["peaks_summits"]
    window = PARAMS["peaks_search_range"]
    chrom_sizes = PARAMS["annotations_chrom_sizes"]
    tmp_dir = "$SCRATCH_DIR"

    statement_start ='''tmp=`mktemp -p %(tmp_dir)s`; checkpoint; 
                        awk 'BEGIN {OFS="\\t"}'''
    
    if summits == "True":
        opts = '''{start=$6-1} {end=$6+1}'''
    else:
        opts = '''{x=($3-$2)/2} {centre=$2+x} {start=centre-1} {end=centre+1} '''

    statement_end = '''{print $1,int(start),int(end),$4}'
                         %(infile)s
                         > $tmp ; checkpoint;
                       slopBed
                         -g %(chrom_sizes)s
                         -b %(window)s
                         -i $tmp |
                       grep -v "chr.*random" - | 
                         grep -v "chrUn.*" - |
                         grep -v "chrM" - 
                         > %(outfile)s; checkpoint;
                       rm $tmp'''
    
    statement = ''.join([statement_start, opts, statement_end])

    P.run()


@follows(offsetPeaks)
@files(None, "readCounts.tsv")
def getReadCounts(infile, outfile):
    '''get total read counts in merged BAMs for normalisation'''

    statement = '''tmp=`mktemp -p %(local_tmpdir)s`; checkpoint;
                   for b in data.dir/*merge.bam; 
                     do name=`echo $b | 
                       sed 's/data.dir\///' | 
                       sed 's/_merge.bam//'`; 
                     samtools idxstats $b | 
                     awk -v name=$name 'BEGIN {OFS="\\t"} 
                       {sum += $3+$4}END{print name,sum}' - 
                       >> $tmp
                     ; done ; checkpoint;
                   mv $tmp %(outfile)s'''

    print(statement)
    
    P.run()

    
@transform(getReadCounts,
           suffix(r".tsv"),
           r".load")
def loadReadCounts(infile, outfile):
    '''load total read counts'''
    
    P.load(infile, outfile, options='-H "sample,total_reads" ')


def coverageBedGenerator():
    '''Create jobs to annotate infiles with motif matches'''

    infiles = glob.glob("coverage.dir/*.window.bed")
    cutsites = glob.glob("cutsites.dir/*.cutsites.bed.gz")

    outdir = "coverage.dir/"

    for c in cutsites:
        for i in infiles:
            c_name = os.path.basename(c).replace(".cutsites.bed.gz", "")
            i_name = os.path.basename(i).replace(".window.bed", "")

            outfile = outdir + '.'.join([i_name, c_name]) + ".bed.gz"

            yield [[i, c], outfile]
            #print([[i, c], outfile])

            
@follows(loadReadCounts, mkdir("coverage.dir"))
@files(coverageBedGenerator)
def coverageBed(infiles, outfile):
    '''Calculate per base coverage of peaks by motif files.
       And correct position to be relative to peak centre, rather than start'''

    bed, cutsite = infiles

    job_memory = "200G" # jobs keep failing with less memory

    statement = '''tmp1=`mktemp -p %(local_tmpdir)s`; checkpoint;
                   tmp2=`mktemp -p %(local_tmpdir)s`; checkpoint;
                   zcat %(cutsite)s > $tmp1; checkpoint;
                   coverageBed 
                     -d 
                     -a %(bed)s                      
                     -b $tmp1 
                     > $tmp2; checkpoint;
                   awk 'BEGIN {OFS="\\t"} 
                     {pwidth=$3-$2}
                     {pcentre=pwidth/2}
                     {if ($5 >= pcentre) print $1,$2,$3,$4,$5-pcentre,$6;
                     else print $1,$2,$3,$4,"-"pcentre-$5,$6}' $tmp2 |
                   gzip -c - > %(outfile)s ; checkpoint;
                   rm $tmp1 $tmp2'''

    P.run()

    
# @transform(coverageBed,
#            suffix(r".bed.gz"),
#            r".bed.norm.gz")
@transform("coverage.dir/*.bed.gz",
           regex(r"(.*).bed.gz"),
           r"\1.bed.norm.gz")
def footprintNorm(infile, outfile):
    '''Normalise ATAC cutting frequency'''

    out = outfile.replace(".gz", "")
    db = PARAMS["database"]
    script = PARAMS["pipeline_dir"] + "footprintNorm.py"

    job_memory = "200G"
    
    statement = '''python %(script)s 
                     --infile %(infile)s 
                     --outfile %(out)s
                     --binsize 1 
                     --database %(db)s ; checkpoint;
                   gzip %(out)s'''

    print(statement)
    
    P.run()
    
    
@transform(footprintNorm,
           regex(r"(.*).bed.norm.gz"),
           r"\1.png")
def plotFootprint(infile, outfile):
    '''Normalise coverage profiles & plot motif enrichment'''

    region_label = os.path.basename(infile).split(".")
    region = region_label[0]
    label = region_label[1]

    window = PARAMS["plot_window"]
    distance = int(window)/2
    smoothed = PARAMS["plot_show_unsmoothed"]
    bandwidth = PARAMS["plot_bandwidth"]
    script = PARAMS["pipeline_dir"] + "plotFootprint.R"
    
    statement = '''Rscript %(script)s
                     --infiles %(infile)s
                     --outfile %(outfile)s
                     --labels %(label)s
                     --title %(region)s 
                     -b %(bandwidth)s
                     --xlims %(distance)s
                     --unsmoothed %(smoothed)s'''

    P.run()


def plotFootprintsGenerator():

    footprints = glob.glob("coverage.dir/*.bed.norm.gz") # get all footprint files
    groups = list(set([os.path.basename(x).split(".")[0] for x in footprints])) # get unique footprint regions for plots

    for g in groups:
        infiles = [x for x in footprints if g in os.path.basename(x).split(".")[0]]
        labels = [os.path.basename(x).split(".")[1].replace(".bed.norm.gz", "") for x in infiles]
        outfile = g + ".all_footprints.png"
        
        yield [infiles, outfile, labels]


@follows(coverageBed)
@files(plotFootprintsGenerator)
def plotFootprints(infiles, outfile, labels):
    '''Plot footprints for all samples over each region'''

    region = ''.join(list(set([os.path.basename(x).split(".")[0] for x in infiles])))
    infiles = ','.join(infiles)
    labels = ','.join(labels)
    
    window = PARAMS["plot_window"]
    distance = int(window)/2
    smoothed = "FALSE" # will be too noisy 
    bandwidth = PARAMS["plot_bandwidth"]
    script = PARAMS["pipeline_dir"] + "plotFootprint.R"
    
    statement = '''Rscript %(script)s
                     --infiles %(infiles)s
                     --outfile %(outfile)s
                     --labels %(labels)s
                     --title %(region)s 
                     -b %(bandwidth)s
                     --xlims %(distance)s
                     --unsmoothed %(smoothed)s'''

    P.run()

    
    
# ---------------------------------------------------
# Generic pipeline tasks
@follows(plotFootprint, plotFootprints)
def full():
    pass


def main(argv=None):
    if argv is None:
        argv = sys.argv
    P.main(argv)


if __name__ == "__main__":
    sys.exit(P.main(sys.argv))    
